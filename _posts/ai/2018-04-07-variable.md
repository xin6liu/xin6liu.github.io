---
layout: post
title: 共享变量
date: 2018-04-07
category: ai
---



```python
import tensorflow as tf
import numpy as np
```


## Recall


```python
tf.ones([3,2], name="t1")
```




    <tf.Tensor 't1:0' shape=(3, 2) dtype=float32>




```python
tf.constant([3,2])
```




    <tf.Tensor 'Const:0' shape=(2,) dtype=int32>




```python
tf.placeholder(tf.int32)
```




    <tf.Tensor 'Placeholder:0' shape=<unknown> dtype=int32>




```python
tf.Variable(5)
```




    <tf.Variable 'Variable:0' shape=() dtype=int32_ref>



## variable用法


```python
var_1 = tf.Variable(1.0, name="haha")
```


```python
print(var_1)
print(var_1.name)
```

    <tf.Variable 'haha:0' shape=() dtype=float32_ref>
    haha:0



```python
var_1 = tf.Variable(2, name="haha")
```


```python
print(var_1)
```

    <tf.Variable 'haha_1:0' shape=() dtype=int32_ref>



```python
init = tf.global_variables_initializer()
```


```python
with tf.Session() as sess:
    sess.run(init)
    print(sess.run(var_1))
```

    2


定义了两个var_1, 因为虽然自己设的名字是一样的haha，但是内存里保存的名字是不一样的（haha:0 和 haha_1:0）。所以内存里有两个var_1, 但是运行sess.run时产生的却是第二个var_1。

## get_variable 用法


```python
with tf.variable_scope("test1"):
    var_2 = tf.get_variable("hehe", shape=[2])    
print(var_2)
```

    <tf.Variable 'test1/hehe:0' shape=(2,) dtype=float32_ref>



```python
with tf.variable_scope("test2"):
    var_2 = tf.get_variable("hehe", shape=[2])    
print(var_2)
```

    <tf.Variable 'test2/hehe:0' shape=(2,) dtype=float32_ref>


可以看出，两个名字是可以一样的，只不过在不同的scope里。

## 共享功能的实现


```python
# scope 嵌套
with tf.variable_scope("t1"):
    v1 = tf.get_variable("n1", shape=[2])
    
    with tf.variable_scope("t2"):
        v2 = tf.get_variable("n1", shape=[2])
        
print(v1)
print(v2)
```

    <tf.Variable 't1/n1:0' shape=(2,) dtype=float32_ref>
    <tf.Variable 't1/t2/n1:0' shape=(2,) dtype=float32_ref>



```python
with tf.variable_scope("t1", reuse=True):
    v3 = tf.get_variable("n1", shape=[2])
    
    with tf.variable_scope("t2", reuse=True):
        v4 = tf.get_variable("n1", shape=[2])
        
print(v3)
print(v4)
```

    <tf.Variable 't1/n1:0' shape=(2,) dtype=float32_ref>
    <tf.Variable 't1/t2/n1:0' shape=(2,) dtype=float32_ref>


v1和v3共享一个变量，v2和v4共享一个变量。

## 共享变量的作用域和初始化


```python
with tf.variable_scope("t1", initializer=tf.constant_initializer(0.4)):
    var1 = tf.get_variable("v1", shape=[2], dtype=tf.float32)
    with tf.variable_scope("t2"):
        var2 = tf.get_variable("v1", shape=[2], dtype=tf.float32)
        var3 = tf.get_variable("var3", shape=[2], initializer=tf.constant_initializer(0.3))
print(var1,"\n",var2,"\n",var3)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(var1))
    print(sess.run(var2))
    print(sess.run(var3))
```

    <tf.Variable 't1/v1:0' shape=(2,) dtype=float32_ref> 
     <tf.Variable 't1/t2/v1:0' shape=(2,) dtype=float32_ref> 
     <tf.Variable 't1/t2/var3:0' shape=(2,) dtype=float32_ref>
    [0.4 0.4]
    [0.4 0.4]
    [0.3 0.3]


## 作用域和操作符的受限范围


```python
with tf.variable_scope("s1") as sp:
    a = tf.get_variable("a",[1])
    
print(sp.name)  # 作用域scope名称
print(a.name)   # 变量variable名称
```

    s1
    s1/a:0



```python
with tf.variable_scope("s2"):
    b = tf.get_variable("a",[1])
    with tf.variable_scope(sp) as sp3:   #注意这里variable_scope()里传送的是sp,而不是命名
        c = tf.get_variable("c",[1])
        
print(sp3.name)
print(b.name)
print(c.name)
```

    s1
    s2/a:0
    s1/c:0


可以看到sp3的name是s1而不是s2, 并不受外层限制。


```python
with tf.variable_scope("s"):
    with tf.name_scope("bar"):
        d = tf.get_variable("d", [1])
        x = 1.0 + d
print(d.name)
print(x.op.name)
```

    s/d:0
    s/bar/add


tf.name_scope 只能限制op,不能限制variable
