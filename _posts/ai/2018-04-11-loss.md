---
layout: post
title: 损失函数
date: 2018-04-11
category: ai
---

损失函数有两个常见的算法：均值平方差(MSE)和交叉熵。

简单来说，loss函数的作用就是：loss值越小，代表预测结果越准。

MSE没啥好说的，每个样本的预测值和实际值的差值进行平方，所有样本加起来再除以样本个数。

交叉熵crossentropy就是，每个样本的实际值乘以ln预测值，加上(1-实际值)乘以ln(1-预测值)，所有样本加起来再除以样本个数。

这里的预测值是经过sigmoid或者softmax处理过的，处于0~1之间，实际值为0或1。

loss函数的选取：当输出的label只有一个数时，用均值平方差；当分类问题，输出的label是一个向量时，使用交叉熵。

TensorFlow中没有直接给出MSE函数，但是因为比较简单，可以自己写：

`MSE = tf.reduce_mean(tf.square(pred-labels))`

softmax交叉熵有现成的：

`tf.nn.softmax_cross_entropy_with_logits(pred, labels)`

当然也可以自己写，当得到pred = tf.nn.softmax([logits1, logits2, logits3]) 之后：

`-tf.reduce_sum(labels * tf.log(pred), 1)`

