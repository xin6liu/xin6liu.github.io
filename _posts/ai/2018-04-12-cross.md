---
layout: post
title: 交叉熵求loss
date: 2018-04-12
category: ai
---


softmax 是激励函数的一种，输出0~1的结果。

cross_entropy 是计算loss的一种方法。（同样的还有MSE, loss越小拟合越好）


```python
import tensorflow as tf
```

    /Users/xinliu/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      from ._conv import register_converters as _register_converters



```python
# laels就是标准答案，一共有3个分类，行数代表样本的个数：2个样本
labels=[[0,0,1],
        [0,1,0]]
# logits就是没有激励前的输出值, 两个样本所以两行
logits = [[2, 0.5, 6],
          [0.1, 0, 3]]
```


```python
a1 = tf.nn.softmax(logits)
a2 = tf.nn.softmax(a1)
```


```python
b1 = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)
b2 = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=a1)
b3 = -tf.reduce_sum(labels*tf.log(a1),1)
```

    WARNING:tensorflow:From <ipython-input-4-7074363bdbe0>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    
    Future major versions of TensorFlow will allow gradients to flow
    into the labels input on backprop by default.
    
    See tf.nn.softmax_cross_entropy_with_logits_v2.
    



```python
with tf.Session() as sess:
    print(sess.run(a1))
    print(sess.run(a2))
    print(sess.run(b1))
    print(sess.run(b2))
    print(sess.run(b3))
```

    [[0.01791432 0.00399722 0.97808844]
     [0.04980332 0.04506391 0.90513283]]
    [[0.21747023 0.21446465 0.56806517]
     [0.2300214  0.22893383 0.5410447 ]]
    [0.02215516 3.0996735 ]
    [0.56551915 1.4743223 ]
    [0.02215518 3.0996735 ]


可以看出来b1和b3是对的。`tf.nn.softmax_cross_entropy_with_logits`已经包括了tf.nn.softmax(logits)的过程。所以传入的数应该是原始的logits而不是softmax过的数据。

第二个样本与label不太像，所以交叉熵比较大。

之前分类问题的标签都使用的标准的one-hot格式，也就是几个0中间有一个1。接下来看如果不是one-hot格式的labels


```python
labels = [[0.4, 0.1, 0.5],
          [0.3, 0.6, 0.1]]
logits = [[2, 0.5, 6],
          [0.1, 0, 3]]
b4 = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)
```


```python
with tf.Session() as sess:
    print(sess.run(b4))
```

    [2.1721554 2.7696736]


发现效果区分并没有one-hot好


```python
# labels=[[0,0,1],[0,1,0]] 可以换成非one-hot label: [2,1]
labels = [2,1]
logits = [[2, 0.5, 6],
          [0.1, 0, 3]]
b5 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)
```


```python
with tf.Session() as sess:
    print(sess.run(b5))
```

    [0.02215516 3.0996735 ]



```python
#发现跟b1和b3效果是一样的
```


```python
# 每个样本的loss加起来就是总loss了
loss = tf.reduce_sum(b1)
with tf.Session() as sess:
    print(sess.run(loss))
```

    3.1218286

