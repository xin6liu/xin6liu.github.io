---
layout: post
title: è¯†åˆ«å›¾ç‰‡
date: 2018-04-17
category: ai
---

ç›®æ ‡ï¼š

1. æ•°æ®é¢„å¤„ç†è„šæœ¬ã€
2. æ•°æ®è¾“å…¥ç½‘ç»œå±‚ã€
3. èƒ½å¤Ÿå¤„ç†æ‰¹é‡æ•°æ®çš„ FullyConnect å±‚ã€
4. æŸå¤±å‡½æ•°å±‚å’Œå‡†ç¡®ç‡å±‚ï¼Œ
5. ä½¿ç”¨è¿™äº›å±‚æ„å»ºå‡ºåªæœ‰**ä¸€ä¸ªéšå±‚**çš„æµ…å±‚ç¥ç»ç½‘ç»œ


```python
!wget http://liuxin21.com/file/data.tar.gz
```

æ²¡æœ‰wgetçš„å¯ä»¥åˆ©ç”¨brewä¸‹è½½ï¼š`brew install wget`


```python
!tar zxvf data.tar.gz
```


```python
!ls
```

    data.tar.gz        test.txt           validate.txt
    [34mpic[m[m                train.txt          è¯†åˆ«å›¾ç‰‡.ipynb


å¯ä»¥çœ‹åˆ°`data.tar.gz`è¢«è§£å‹æˆä¸€ä¸ªpicæ–‡ä»¶å¤¹å’Œä¸‰ä¸ªtxtæ–‡ä»¶ã€‚


```python
! cat train.txt
```

ä¾‹å¦‚ç¬¬ä¸€ä¸ªå›¾ç‰‡ 0.png æ˜¾ç¤ºçš„æ˜¯å­—æ¯D, train.txt é‡Œå¯¹åº”çš„å°±æ˜¯ 3ï¼›ç¬¬äºŒä¸ªå›¾ç‰‡ 1.png é‡Œé¢ç”»æ˜¯å­—æ¯Xï¼Œtrain.txt é‡Œå¯¹åº”çš„å°±æ˜¯ 23.

1. `train.txt`: è®­ç»ƒé›† (0.png - 39999.png)
2. `validate.txt`: éªŒè¯é›† (40000.png - 49999.png)
3. `test.txt`: æµ‹è¯•é›† (50000.png - 59999.png)

(åœ¨sklearnä¸­ï¼Œå¯ä»¥ç”¨`from sklearn.model_selection import train_test_split`éšæœºæŠŠæ ·æœ¬åˆ†æˆtrainå’Œtest)

(å› ä¸ºæ˜¯ç›‘ç£å­¦ä¹ ï¼Œè¿™äº›txtæ–‡ä»¶å°±æ˜¯å›¾ç‰‡æ–‡ä»¶çš„æ ‡ç­¾)

## é¢„å¤„ç†å›¾ç‰‡

å¯¹äºå›¾ç‰‡æ•°æ®ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å°†å®ƒä»¬è½¬æ¢æˆè¾“å…¥å‘é‡çš„å½¢å¼ï¼Œå¹¶ä¸”ç”±äºæˆ‘ä»¬æ˜¯æœ‰ç›‘ç£å­¦ä¹ ï¼Œæ¯å¼ å›¾ç‰‡çš„æ ‡ç­¾ä¹Ÿå¿…é¡»ä¸å¯¹åº”çš„å›¾ç‰‡å‘é‡ä¸€ä¸€å¯¹åº”ã€‚


```python
from scipy import misc
import numpy as np
```


```python
def main(src, dst):
    '''
    src: train.txtã€validate.txtå’Œtest.txtï¼Œ
    æˆ‘ä»¬ä»srcä¸­è¯»å–å›¾ç‰‡çš„è·¯å¾„å’Œå®ƒçš„æ ‡ç­¾
    dst: ä»£è¡¨æˆ‘ä»¬å°†é¢„å¤„ç†å¥½çš„å›¾ç‰‡æ•°æ®ä¿å­˜åˆ°å“ªé‡Œï¼Œ
    æˆ‘ä»¬ç›´æ¥ä½¿ç”¨ np.save() å‡½æ•°å°†æ•°ç»„ä¿å­˜åˆ°npyæ–‡ä»¶ã€‚
    '''
    with open(src, 'r') as f:
        list = f.readlines()
        
    data = []
    labels = []
    for i in list:
        name, label = i.strip('\n').split(' ')  # å°†å›¾ç‰‡åˆ—è¡¨ä¸­çš„æ¯ä¸€è¡Œæ‹†åˆ†æˆå›¾ç‰‡åå’Œå›¾ç‰‡æ ‡ç­¾
        print("name: ", name, ", label: ", label, ', processed')
        img = misc.imread(name) # å°†å›¾ç‰‡è¯»å–å‡ºæ¥ï¼Œå­˜å…¥ä¸€ä¸ªçŸ©é˜µ
        img = img/255 # å°†å›¾ç‰‡è½¬æ¢ä¸ºåªæœ‰0ã€1å€¼çš„çŸ©é˜µ
        img.resize((img.size, 1))  # ä¸ºäº†ä¹‹åçš„è¿ç®—æ–¹ä¾¿ï¼Œæˆ‘ä»¬å°†å›¾ç‰‡å­˜å‚¨åˆ°ä¸€ä¸ªimg.size*1çš„åˆ—å‘é‡é‡Œé¢
        data.append(img)
        labels.append(int(label))

    np.save(dst, [data, labels])  # å°†è®­ç»ƒæ•°æ®ä»¥npyçš„å½¢å¼ä¿å­˜åˆ°æˆæœ¬åœ°æ–‡ä»¶
```


```python
main('train.txt','train.npy')
main('validate.txt','validate.npy')
main('test.txt','test.npy')
```

    name:  pic/0.png , label:  3 , processed
    name:  pic/1.png , label:  23 , processed
    name:  pic/2.png , label:  12 , processed
    name:  pic/3.png , label:  4 , processed
    name:  pic/4.png , label:  22 , processed
    name:  pic/5.png , label:  2 , processed
    ...
    name:  pic/59998.png , label:  0 , processed
    name:  pic/59999.png , label:  1 , processed


## æ•°æ®å±‚

å°†æ•°æ®è¯»å…¥æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œï¼Œä¸ºäº†ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å°†è¯»å…¥æ•°æ®çš„æ“ä½œæ”¾åˆ°ä¸€ä¸ªæ•°æ®å±‚é‡Œé¢ï¼Œæ•°æ®å±‚ä»£ç å¦‚ä¸‹ï¼š


```python
class Data:
    '''
    è¾“å…¥: nameï¼Œbatch_size
    eg: ä½¿ç”¨æ—¶ï¼ŒData("xxx.txt", 1000).backward(d)
    åˆå§‹åŒ–: (x,y,l,batch_size,pos)
    
    '''
    def __init__(self, name, batch_size):  # æ•°æ®æ‰€åœ¨çš„æ–‡ä»¶ånameå’Œbatchä¸­å›¾ç‰‡çš„æ•°é‡batch_size
        with open(name, 'rb') as f:
            data = np.load(f)
        self.x = data[0]  # è¾“å…¥x
        self.y = data[1]  # é¢„æœŸæ­£ç¡®è¾“å‡ºy
        self.l = len(self.x)
        self.batch_size = batch_size
        self.pos = 0  # posç”¨æ¥è®°å½•æ•°æ®è¯»å–çš„ä½ç½®

    def forward(self):
        pos = self.pos  
        bat = self.batch_size
        l = self.l
        if pos + bat >= l:  # å·²ç»æ˜¯æœ€åä¸€ä¸ªbatchæ—¶ï¼Œè¿”å›å‰©ä½™çš„æ•°æ®ï¼Œå¹¶è®¾ç½®posä¸ºå¼€å§‹ä½ç½®0
            ret = (self.x[pos:l], self.y[pos:l])
            self.pos = 0
            index = range(l)
            np.random.shuffle(list(index))  # å°†è®­ç»ƒæ•°æ®æ‰“ä¹±
            self.x = self.x[index]
            self.y = self.y[index]
        else:  # ä¸æ˜¯æœ€åä¸€ä¸ªbatch, posç›´æ¥åŠ ä¸Šbatch_size
            ret = (self.x[pos:pos + bat], self.y[pos:pos + bat])
            self.pos += self.batch_size

        return ret, self.pos  # è¿”å›çš„posä¸º0æ—¶ä»£è¡¨ä¸€ä¸ªepochå·²ç»ç»“æŸ

    def backward(self, d):  # æ•°æ®å±‚æ— backwardæ“ä½œ
        pass
```

éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆstochastic gradient descentï¼‰:

åœ¨å®é™…çš„æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹å½“ä¸­ï¼Œæˆ‘ä»¬æ¯æ¬¡è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°å€¼æ—¶ï¼Œæ€»æ˜¯ä¸€æ¬¡æ€§è®¡ç®—å¤šä¸ªè¾“å…¥æ•°æ®çš„æ¢¯åº¦ï¼Œå¹¶å°†è¿™äº›æ¢¯åº¦æ±‚å¹³å‡å€¼ï¼Œå†ä½¿ç”¨è¿™ä¸ªå¹³å‡å€¼å¯¹å‚æ•°è¿›è¡Œæ›´æ–°ã€‚è¿™æ ·åšå¯ä»¥åˆ©ç”¨å¹¶è¡Œè®¡ç®—æ¥æé«˜è®­ç»ƒé€Ÿåº¦ã€‚æˆ‘ä»¬å°†ä¸€æ¬¡æ€§ä¸€èµ·è®¡ç®—çš„ä¸€ç»„æ•°æ®ç§°ä¸ºä¸€ä¸ª`batch`ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ç§°æ‰€æœ‰è®­ç»ƒå›¾ç‰‡éƒ½å·²å‚ä¸ä¸€éè®­ç»ƒçš„ä¸€ä¸ªå‘¨æœŸç§°ä¸ºä¸€ä¸ª`epoch`ã€‚æ¯ä¸ª`epoch`ç»“æŸæ—¶ï¼Œæˆ‘ä»¬ä¼šå°†è®­ç»ƒæ•°æ®é‡æ–°æ‰“ä¹±ï¼Œè¿™æ ·å¯ä»¥è·å¾—æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚æˆ‘ä»¬é€šå¸¸ä¼šè®­ç»ƒå¤šä¸ª`epoch`ã€‚

## å…¨è¿æ¥å±‚


```python
class FullyConnect:
    def __init__(self, l_x, l_y):  # ä¸¤ä¸ªå‚æ•°åˆ†åˆ«ä¸ºè¾“å…¥å±‚çš„é•¿åº¦å’Œè¾“å‡ºå±‚çš„é•¿åº¦
        self.weights = np.random.randn(l_y, l_x) / np.sqrt(l_x)  # ä½¿ç”¨éšæœºæ•°åˆå§‹åŒ–å‚æ•°ï¼Œè¯·æš‚æ—¶å¿½ç•¥è¿™é‡Œä¸ºä»€ä¹ˆå¤šäº†np.sqrt(l_x)
        self.bias = np.random.randn(l_y, 1)  # ä½¿ç”¨éšæœºæ•°åˆå§‹åŒ–å‚æ•°
        self.lr = 0  # å…ˆå°†å­¦ä¹ é€Ÿç‡åˆå§‹åŒ–ä¸º0ï¼Œæœ€åç»Ÿä¸€è®¾ç½®å­¦ä¹ é€Ÿç‡

    def forward(self, x):
        self.x = x  # æŠŠä¸­é—´ç»“æœä¿å­˜ä¸‹æ¥ï¼Œä»¥å¤‡åå‘ä¼ æ’­æ—¶ä½¿ç”¨
        self.y = np.array([np.dot(self.weights, xx) + self.bias for xx in x])  # è®¡ç®—å…¨è¿æ¥å±‚çš„è¾“å‡º
        return self.y  # å°†è¿™ä¸€å±‚è®¡ç®—çš„ç»“æœå‘å‰ä¼ é€’

    def backward(self, d):
        ddw = [np.dot(dd, xx.T) for dd, xx in zip(d, self.x)]  # æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œå°†åå‘ä¼ é€’å›æ¥çš„å¯¼æ•°å€¼ä¹˜ä»¥xï¼Œå¾—åˆ°å¯¹å‚æ•°çš„æ¢¯åº¦
        self.dw = np.sum(ddw, axis=0) / self.x.shape[0]
        self.db = np.sum(d, axis=0) / self.x.shape[0]
        self.dx = np.array([np.dot(self.weights.T, dd) for dd in d])

        # æ›´æ–°å‚æ•°
        self.weights -= self.lr * self.dw
        self.bias -= self.lr * self.db
        return self.dx  # åå‘ä¼ æ’­æ¢¯åº¦
```

ä¸ºäº†ç†è§£ä¸Šé¢çš„ä»£ç ï¼Œæˆ‘ä»¬ä»¥ä¸€ä¸ªåŒ…å« 100 ä¸ªè®­ç»ƒè¾“å…¥æ•°æ®çš„ batch ä¸ºä¾‹ï¼Œåˆ†æä¸€ä¸‹å…·ä½“æ‰§è¡Œæµç¨‹ï¼š
æˆ‘ä»¬çš„ l_x ä¸ºè¾“å…¥å•ä¸ªæ•°æ®å‘é‡çš„é•¿åº¦ï¼Œåœ¨è¿™é‡Œæ˜¯ 17*17=289ï¼Œl_y ä»£è¡¨å…¨è¿æ¥å±‚è¾“å‡ºçš„èŠ‚ç‚¹æ•°é‡ï¼Œç”±äºå¤§å†™è‹±æ–‡å­—æ¯æœ‰ 26 ä¸ªï¼Œæ‰€ä»¥è¿™é‡Œçš„ l_y=26ã€‚
æ‰€ä»¥ï¼Œæˆ‘ä»¬çš„ self.weights çš„å°ºå¯¸ä¸º 26*289, self.bias çš„å°ºå¯¸ä¸º 26*1ï¼ˆself.bias ä¹Ÿæ˜¯é€šè¿‡çŸ©é˜µå½¢å¼è¡¨ç¤ºçš„å‘é‡ï¼‰ã€‚forward() å‡½æ•°çš„è¾“å…¥ x åœ¨è¿™é‡Œçš„å°ºå¯¸å°±æ˜¯ 100*289*1(batch_size * å‘é‡é•¿åº¦ * 1)ã€‚backward() å‡½æ•°çš„è¾“å…¥ d ä»£è¡¨ä»å‰é¢çš„ç½‘ç»œå±‚åå‘ä¼ é€’å›æ¥çš„ â€œéƒ¨åˆ†æ¢¯åº¦å€¼â€ï¼Œå…¶å°ºå¯¸ä¸º 100*26*1ï¼ˆbatch_size * è¾“å‡ºå±‚èŠ‚ç‚¹æ•° l_y*1ï¼‰ã€‚

forward() å‡½æ•°é‡Œçš„ä»£ç æ¯”è¾ƒå¥½ç†è§£ï¼Œç”±äºè¿™é‡Œçš„ x åŒ…å«äº†å¤šç»„æ•°æ®ï¼Œæ‰€ä»¥è¦å¯¹æ¯ç»„æ•°æ®åˆ†åˆ«è¿›è¡Œè®¡ç®—ã€‚

backward() å‡½æ•°é‡Œçš„ä»£ç å°±ä¸å¤ªå¥½ç†è§£äº†ï¼Œddw ä¿å­˜çš„æ˜¯å¯¹äºæ¯ç»„è¾“å…¥æ•°æ®ï¼ŒæŸå¤±å‡½æ•°å¯¹äºå‚æ•°çš„æ¢¯åº¦ã€‚ç”±äºè¿™é‡Œçš„å‚æ•°æ˜¯ä¸€ä¸ª 26*289 çš„çŸ©é˜µï¼Œæ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦æ±‚æŸå¤±å‡½æ•°å¯¹çŸ©é˜µçš„å¯¼æ•°ã€‚ï¼ˆå¯¹çŸ©é˜µæ±‚å¯¼å¯èƒ½å¤§éƒ¨åˆ†æœ¬ç§‘ç”Ÿéƒ½ä¸ä¼šã€‚ä½†å…¶å®ä¹Ÿä¸éš¾ï¼Œå¦‚æœä½ çº¿æ€§ä»£æ•°åŠŸåº•å¯ä»¥ï¼Œå¯ä»¥å°è¯•æ¨å¯¼çŸ©é˜µæ±‚å¯¼å…¬å¼ã€‚ï¼‰ä¸è¿‡è¿™é‡Œæœ‰ä¸€ä¸ªç®€ä¾¿çš„æ–¹æ³•å»æ¨æ–­å¯¹çŸ©é˜µæ±‚å¯¼æ—¶åº”è¯¥å¦‚ä½•è®¡ç®—ï¼šç”±äºè¿™é‡Œçš„å‚æ•°çŸ©é˜µæœ¬èº«æ˜¯ 26*289 çš„ï¼Œé‚£æŸå¤±å‡½æ•°å¯¹äºå®ƒçš„æ¢¯åº¦ï¼ˆå³æŸå¤±å‡½æ•°å¯¹å‚æ•°çŸ©é˜µæ±‚å¯¼çš„ç»“æœï¼‰çš„å°ºå¯¸ä¹Ÿä¸€å®šæ˜¯ 26*289 çš„ã€‚è€Œè¿™é‡Œæ¯ç»„è¾“å…¥æ•°æ®çš„å°ºå¯¸æ˜¯ 289*1ï¼Œæ¯ç»„æ•°æ®å¯¹åº”çš„éƒ¨åˆ†æ¢¯åº¦å°ºå¯¸ä¸º 26*1, è¦å¾—åˆ°ä¸€ä¸ª 26*289 å°ºå¯¸çš„æ¢¯åº¦çŸ©é˜µï¼Œå°±åªèƒ½æ˜¯ä¸€ä¸ª 26*1 å°ºå¯¸çš„çŸ©é˜µä¹˜ä»¥ä¸€ä¸ª 1*289 å°ºå¯¸çš„çŸ©é˜µï¼Œéœ€è¦å¯¹è¾“å…¥æ•°æ®è¿›è¡Œè½¬ç½®ã€‚æ‰€ä»¥è¿™é‡Œè®¡ç®—çš„æ˜¯`np.dot(dd,xx.T)`ã€‚
å¯¹ä¸€ä¸ª batch é‡Œçš„æ•°æ®åˆ†åˆ«æ±‚å¾—æ¢¯åº¦ä¹‹åï¼ŒæŒ‰ç…§`éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•`çš„è¦æ±‚ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ‰€æœ‰æ¢¯åº¦æ±‚å¹³å‡å€¼ï¼Œå¾—åˆ° self.dw, å…¶å°ºå¯¸ä¸º 26*289ï¼Œåˆšå¥½ä¸æˆ‘ä»¬çš„ self.weights åŒ¹é…ã€‚

ç”±äºå…¨è¿æ¥å±‚å¯¹ bias çš„éƒ¨åˆ†å¯¼æ•°ä¸º 1ï¼Œæ‰€ä»¥è¿™é‡Œå¯¹äº bias çš„æ¢¯åº¦ self.bias å°±ç›´æ¥ç­‰äºä»ä¹‹å‰çš„å±‚åå‘ä¼ å›æ¥çš„æ¢¯åº¦çš„å¹³å‡å€¼ã€‚
æŸå¤±å‡½æ•°å¯¹äºè¾“å…¥ x çš„æ¢¯åº¦å€¼ self.dx çš„æ±‚è§£ä¸ self.dw ç±»ä¼¼ã€‚ç”±äºè¾“å…¥æ•°æ® self.x ä¸­çš„ä¸€ä¸ªæ•°æ®çš„å°ºå¯¸ä¸º 289*1ï¼Œself.weights çš„å°ºå¯¸ä¸º 26*289, dd çš„å°ºå¯¸ä¸º 26*1, æ‰€ä»¥éœ€è¦å¯¹ self.weights è¿›è¡Œè½¬ç½®ã€‚å³ â€œ289*1=(289*26)*(26*1)â€ã€‚

æœ€åæ˜¯ä½¿ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°ï¼Œæ³¨æ„è¿™é‡Œçš„ self.lr å³ä¸ºå‰é¢æˆ‘ä»¬æåˆ°è¿‡çš„å­¦ä¹ é€Ÿç‡`alpha`ï¼Œå®ƒæ˜¯ä¸€ä¸ªéœ€è¦æˆ‘ä»¬æ‰‹å·¥è®¾å®šçš„è¶…å‚æ•°ã€‚

è¿™é‡Œçš„çŸ©é˜µæ±‚å¯¼ç¡®å®ä¸å¤ªå¥½å¤„ç†ï¼Œå®¹æ˜“å‡ºé”™ï¼Œè¯·ä½ ä»”ç»†åˆ†ææ¯ä¸€ä¸ªå˜é‡ä»£è¡¨çš„å«ä¹‰ï¼Œå¦‚æœå¯¹ä¸€ä¸ªåœ°æ–¹ä¸æ¸…æ¥šï¼Œè¯·å›åˆ°å‰é¢çœ‹çœ‹ç›¸å…³çš„æ¦‚å¿µæ˜¯å¦‚ä½•å®šä¹‰çš„ã€‚

## æ¿€æ´»å‡½æ•°å±‚


```python
class Sigmoid:
    def __init__(self):  # æ— å‚æ•°ï¼Œä¸éœ€åˆå§‹åŒ–
        pass

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def forward(self, x):
        self.x = x
        self.y = self.sigmoid(x)
        return self.y

    def backward(self, d):
        sig = self.sigmoid(self.x)
        self.dx = d * sig * (1 - sig)
        return self.dx  # åå‘ä¼ é€’æ¢¯åº¦
```

sigmoid å‡½æ•°å°†è¾“å‡ºé™åˆ¶åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œåˆšå¥½å¯ä»¥ä½œä¸ºæ¦‚ç‡çœ‹å¾…ã€‚è¿™é‡Œæˆ‘ä»¬æœ‰ 26 ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œç»è¿‡ sigmoid å±‚è®¡ç®—ä¹‹åï¼Œå“ªä¸ªè¾“å‡ºèŠ‚ç‚¹çš„æ•°å€¼æœ€å¤§ï¼Œå°±è®¤ä¸ºå›¾ç‰‡ä¸Šæœ€æœ‰å¯èƒ½æ˜¯è¯¥èŠ‚ç‚¹ä»£è¡¨çš„å­—æ¯ã€‚æ¯”å¦‚å¦‚æœè¾“å‡ºå±‚ç¬¬ 0 ä¸ªèŠ‚ç‚¹å€¼æœ€å¤§ï¼Œå°±è®¤ä¸ºå›¾ç‰‡ä¸Šçš„å­—æ¯æ˜¯ â€œAâ€, å¦‚æœç¬¬ 25 ä¸ªèŠ‚ç‚¹çš„å€¼æœ€å¤§ï¼Œå°±è®¤ä¸ºå›¾ç‰‡ä¸Šçš„å­—æ¯æ˜¯ â€œZâ€ã€‚

æ³¨æ„ä¸€èˆ¬åœ¨è®¡ç®—ç¥ç»ç½‘ç»œçš„æ·±åº¦æ—¶æˆ‘ä»¬ä¸€èˆ¬ä¸æŠŠæ¿€æ´»å±‚ç®—è¿›å»ï¼Œä½†è¿™é‡Œä¸ºäº†ç¼–ç¨‹æ–¹ä¾¿ï¼Œä¹Ÿå°†æ¿€æ´»å‡½æ•°è§†ä¸ºå•ç‹¬çš„ä¸€å±‚ã€‚ã€€ã€€

## æŸå¤±å‡½æ•°å±‚


```python
class QuadraticLoss:
    def __init__(self):
        pass

    def forward(self, x, label):
        self.x = x
        self.label = np.zeros_like(x)  # ç”±äºæˆ‘ä»¬çš„labelæœ¬èº«åªåŒ…å«ä¸€ä¸ªæ•°å­—ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢æˆå’Œæ¨¡å‹è¾“å‡ºå€¼å°ºå¯¸ç›¸åŒ¹é…çš„å‘é‡å½¢å¼
        for a, b in zip(self.label, label):
            a[b] = 1.0  # åªæœ‰æ­£ç¡®æ ‡ç­¾æ‰€ä»£è¡¨çš„ä½ç½®æ¦‚ç‡ä¸º1ï¼Œå…¶ä»–ä¸º0
        self.loss = np.sum(np.square(x - self.label)) / self.x.shape[0] / 2  # æ±‚å¹³å‡åå†é™¤ä»¥2æ˜¯ä¸ºäº†è¡¨ç¤ºæ–¹ä¾¿
        return self.loss

    def backward(self):
        self.dx = (self.x - self.label) / self.x.shape[0]  # 2è¢«æŠµæ¶ˆæ‰äº†
        return self.dx
```

åœ¨`éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•`é‡Œï¼Œæ¯æ¬¡å‰å‘è®¡ç®—å’Œåå‘ä¼ æ’­éƒ½ä¼šè®¡ç®—åŒ…å«å¤šä¸ªè¾“å…¥æ•°æ®çš„ä¸€ä¸ª batchã€‚æ‰€ä»¥æŸå¤±å‡½æ•°å€¼åœ¨éšåä¹Ÿè¦é™¤ä»¥ batch ä¸­åŒ…å«çš„æ•°æ®æ•°é‡,ã€€å³`self.x.shape[0]`ï¼ŒåŒæ—¶è¿™é‡Œé™¤ä»¥äº† 2,ã€€è¿™ä¸ªåœ°æ–¹çš„ 2 å¯ä»¥å’Œå¯¹äºŒæ¬¡æŸå¤±å‡½æ•°æ±‚å¯¼åå¤šå‡ºæ¥çš„ç³»æ•° 2 æŠµæ¶ˆæ‰ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬çš„æŸå¤±å‡½æ•°å˜æˆäº†ï¼š

$$
J(\theta) = \frac{1}{2m} \sum^m_{i=1} (h(\theta,X_i)-Y_i)^2
$$

## å‡†ç¡®ç‡å±‚


```python
class Accuracy:
    def __init__(self):
        pass

    def forward(self, x, label):  # åªéœ€forward
        self.accuracy = np.sum([np.argmax(xx) == ll for xx, ll in zip(x, label)])  # å¯¹é¢„æµ‹æ­£ç¡®çš„å®ä¾‹æ•°æ±‚å’Œ
        self.accuracy = 1.0 * self.accuracy / x.shape[0]
        return self.accuracy
```

å¦‚æœæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚ä¸­ï¼Œæ¦‚ç‡æœ€å¤§çš„èŠ‚ç‚¹çš„ä¸‹æ ‡ä¸å®é™…çš„æ ‡ç­¾ label ç›¸ç­‰ï¼Œåˆ™é¢„æµ‹æ­£ç¡®ã€‚é¢„æµ‹æ­£ç¡®çš„æ•°é‡é™¤ä»¥æ€»çš„æ•°é‡ï¼Œå°±å¾—åˆ°äº†æ­£ç¡®ç‡ã€‚

## æ„å»ºç¥ç»ç½‘ç»œ


```python
def main():
    datalayer1 = Data('train.npy', 1024)  # ç”¨äºè®­ç»ƒï¼Œbatch_sizeè®¾ç½®ä¸º1024
    datalayer2 = Data('validate.npy', 10000)  # ç”¨äºéªŒè¯ï¼Œæ‰€ä»¥è®¾ç½®batch_sizeä¸º10000,ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰çš„æ ·ä¾‹
    inner_layers = []
    inner_layers.append(FullyConnect(17 * 17, 26))
    inner_layers.append(Sigmoid())
    losslayer = QuadraticLoss()
    accuracy = Accuracy()

    for layer in inner_layers:
        layer.lr = 1000.0  # ä¸ºæ‰€æœ‰ä¸­é—´å±‚è®¾ç½®å­¦ä¹ é€Ÿç‡

    epochs = 20
    for i in range(epochs):
        print('epochs:', i)
        losssum = 0
        iters = 0
        while True:
            data, pos = datalayer1.forward()  # ä»æ•°æ®å±‚å–å‡ºæ•°æ®
            x, label = data
            for layer in inner_layers:  # å‰å‘è®¡ç®—
                x = layer.forward(x)

            loss = losslayer.forward(x, label)  # è°ƒç”¨æŸå¤±å±‚forwardå‡½æ•°è®¡ç®—æŸå¤±å‡½æ•°å€¼
            losssum += loss
            iters += 1
            d = losslayer.backward()  # è°ƒç”¨æŸå¤±å±‚backwardå‡½æ•°å±‚è®¡ç®—å°†è¦åå‘ä¼ æ’­çš„æ¢¯åº¦

            for layer in inner_layers[::-1]:  # åå‘ä¼ æ’­
                d = layer.backward(d)

            if pos == 0:  # ä¸€ä¸ªepochå®Œæˆåè¿›è¡Œå‡†ç¡®ç‡æµ‹è¯•
                data, _ = datalayer2.forward()
                x, label = data
                for layer in inner_layers:
                    x = layer.forward(x)
                accu = accuracy.forward(x, label)  # è°ƒç”¨å‡†ç¡®ç‡å±‚forward()å‡½æ•°æ±‚å‡ºå‡†ç¡®ç‡
                print('loss:', losssum / iters)
                print('accuracy:', accu)
                break

if __name__ == '__main__':
    main()
```

    epochs: 0
    loss: 0.541326932161
    accuracy: 0.33
    epochs: 1
    loss: 0.322561570007
    accuracy: 0.549
    epochs: 2
    loss: 0.24825038484
    accuracy: 0.5959
    epochs: 3
    loss: 0.214456080778
    accuracy: 0.6824
    epochs: 4
    loss: 0.179479788139
    accuracy: 0.7511
    epochs: 5
    loss: 0.152682180959
    accuracy: 0.7738
    epochs: 6
    loss: 0.134862201922
    accuracy: 0.7989
    epochs: 7
    loss: 0.123318053388
    accuracy: 0.8113
    epochs: 8
    loss: 0.115694421735
    accuracy: 0.8242
    epochs: 9
    loss: 0.110088927924
    accuracy: 0.831
    epochs: 10
    loss: 0.105729115814
    accuracy: 0.8361
    epochs: 11
    loss: 0.102193523711
    accuracy: 0.8394
    epochs: 12
    loss: 0.0992462051966
    accuracy: 0.8528
    epochs: 13
    loss: 0.0967390187206
    accuracy: 0.8541
    epochs: 14
    loss: 0.0945715467217
    accuracy: 0.8566
    epochs: 15
    loss: 0.0926725451782
    accuracy: 0.8584
    epochs: 16
    loss: 0.0909897902294
    accuracy: 0.8596
    epochs: 17
    loss: 0.0894841478902
    accuracy: 0.872
    epochs: 18
    loss: 0.0881259172677
    accuracy: 0.8724
    epochs: 19
    loss: 0.0868923293202
    accuracy: 0.8733



```python

```
