---
layout: post
title: 识别图片
date: 2018-04-17
category: ai
---

目标：

1. 数据预处理脚本、
2. 数据输入网络层、
3. 能够处理批量数据的 FullyConnect 层、
4. 损失函数层和准确率层，
5. 使用这些层构建出只有**一个隐层**的浅层神经网络


```python
!wget http://liuxin21.com/file/data.tar.gz
```

没有wget的可以利用brew下载：`brew install wget`


```python
!tar zxvf data.tar.gz
```


```python
!ls
```

    data.tar.gz        test.txt           validate.txt
    [34mpic[m[m                train.txt          识别图片.ipynb


可以看到`data.tar.gz`被解压成一个pic文件夹和三个txt文件。


```python
! cat train.txt
```

例如第一个图片 0.png 显示的是字母D, train.txt 里对应的就是 3；第二个图片 1.png 里面画是字母X，train.txt 里对应的就是 23.

1. `train.txt`: 训练集 (0.png - 39999.png)
2. `validate.txt`: 验证集 (40000.png - 49999.png)
3. `test.txt`: 测试集 (50000.png - 59999.png)

(在sklearn中，可以用`from sklearn.model_selection import train_test_split`随机把样本分成train和test)

(因为是监督学习，这些txt文件就是图片文件的标签)

## 预处理图片

对于图片数据，我们首先需要将它们转换成输入向量的形式，并且由于我们是有监督学习，每张图片的标签也必须与对应的图片向量一一对应。


```python
from scipy import misc
import numpy as np
```


```python
def main(src, dst):
    '''
    src: train.txt、validate.txt和test.txt，
    我们从src中读取图片的路径和它的标签
    dst: 代表我们将预处理好的图片数据保存到哪里，
    我们直接使用 np.save() 函数将数组保存到npy文件。
    '''
    with open(src, 'r') as f:
        list = f.readlines()
        
    data = []
    labels = []
    for i in list:
        name, label = i.strip('\n').split(' ')  # 将图片列表中的每一行拆分成图片名和图片标签
        print("name: ", name, ", label: ", label, ', processed')
        img = misc.imread(name) # 将图片读取出来，存入一个矩阵
        img = img/255 # 将图片转换为只有0、1值的矩阵
        img.resize((img.size, 1))  # 为了之后的运算方便，我们将图片存储到一个img.size*1的列向量里面
        data.append(img)
        labels.append(int(label))

    np.save(dst, [data, labels])  # 将训练数据以npy的形式保存到成本地文件
```


```python
main('train.txt','train.npy')
main('validate.txt','validate.npy')
main('test.txt','test.npy')
```

    name:  pic/0.png , label:  3 , processed
    name:  pic/1.png , label:  23 , processed
    name:  pic/2.png , label:  12 , processed
    name:  pic/3.png , label:  4 , processed
    name:  pic/4.png , label:  22 , processed
    name:  pic/5.png , label:  2 , processed
    ...
    name:  pic/59998.png , label:  0 , processed
    name:  pic/59999.png , label:  1 , processed


## 数据层

将数据读入我们的神经网络，为了一致性，我们将读入数据的操作放到一个数据层里面，数据层代码如下：


```python
class Data:
    '''
    输入: name，batch_size
    eg: 使用时，Data("xxx.txt", 1000).backward(d)
    初始化: (x,y,l,batch_size,pos)
    
    '''
    def __init__(self, name, batch_size):  # 数据所在的文件名name和batch中图片的数量batch_size
        with open(name, 'rb') as f:
            data = np.load(f)
        self.x = data[0]  # 输入x
        self.y = data[1]  # 预期正确输出y
        self.l = len(self.x)
        self.batch_size = batch_size
        self.pos = 0  # pos用来记录数据读取的位置

    def forward(self):
        pos = self.pos  
        bat = self.batch_size
        l = self.l
        if pos + bat >= l:  # 已经是最后一个batch时，返回剩余的数据，并设置pos为开始位置0
            ret = (self.x[pos:l], self.y[pos:l])
            self.pos = 0
            index = range(l)
            np.random.shuffle(list(index))  # 将训练数据打乱
            self.x = self.x[index]
            self.y = self.y[index]
        else:  # 不是最后一个batch, pos直接加上batch_size
            ret = (self.x[pos:pos + bat], self.y[pos:pos + bat])
            self.pos += self.batch_size

        return ret, self.pos  # 返回的pos为0时代表一个epoch已经结束

    def backward(self, d):  # 数据层无backward操作
        pass
```

随机梯度下降算法（stochastic gradient descent）:

在实际的深度学习训练过程当中，我们每次计算梯度并更新参数值时，总是一次性计算多个输入数据的梯度，并将这些梯度求平均值，再使用这个平均值对参数进行更新。这样做可以利用并行计算来提高训练速度。我们将一次性一起计算的一组数据称为一个`batch`。同时，我们称所有训练图片都已参与一遍训练的一个周期称为一个`epoch`。每个`epoch`结束时，我们会将训练数据重新打乱，这样可以获得更好的训练效果。我们通常会训练多个`epoch`。

## 全连接层


```python
class FullyConnect:
    def __init__(self, l_x, l_y):  # 两个参数分别为输入层的长度和输出层的长度
        self.weights = np.random.randn(l_y, l_x) / np.sqrt(l_x)  # 使用随机数初始化参数，请暂时忽略这里为什么多了np.sqrt(l_x)
        self.bias = np.random.randn(l_y, 1)  # 使用随机数初始化参数
        self.lr = 0  # 先将学习速率初始化为0，最后统一设置学习速率

    def forward(self, x):
        self.x = x  # 把中间结果保存下来，以备反向传播时使用
        self.y = np.array([np.dot(self.weights, xx) + self.bias for xx in x])  # 计算全连接层的输出
        return self.y  # 将这一层计算的结果向前传递

    def backward(self, d):
        ddw = [np.dot(dd, xx.T) for dd, xx in zip(d, self.x)]  # 根据链式法则，将反向传递回来的导数值乘以x，得到对参数的梯度
        self.dw = np.sum(ddw, axis=0) / self.x.shape[0]
        self.db = np.sum(d, axis=0) / self.x.shape[0]
        self.dx = np.array([np.dot(self.weights.T, dd) for dd in d])

        # 更新参数
        self.weights -= self.lr * self.dw
        self.bias -= self.lr * self.db
        return self.dx  # 反向传播梯度
```

为了理解上面的代码，我们以一个包含 100 个训练输入数据的 batch 为例，分析一下具体执行流程：
我们的 l_x 为输入单个数据向量的长度，在这里是 17*17=289，l_y 代表全连接层输出的节点数量，由于大写英文字母有 26 个，所以这里的 l_y=26。
所以，我们的 self.weights 的尺寸为 26*289, self.bias 的尺寸为 26*1（self.bias 也是通过矩阵形式表示的向量）。forward() 函数的输入 x 在这里的尺寸就是 100*289*1(batch_size * 向量长度 * 1)。backward() 函数的输入 d 代表从前面的网络层反向传递回来的 “部分梯度值”，其尺寸为 100*26*1（batch_size * 输出层节点数 l_y*1）。

forward() 函数里的代码比较好理解，由于这里的 x 包含了多组数据，所以要对每组数据分别进行计算。

backward() 函数里的代码就不太好理解了，ddw 保存的是对于每组输入数据，损失函数对于参数的梯度。由于这里的参数是一个 26*289 的矩阵，所以，我们需要求损失函数对矩阵的导数。（对矩阵求导可能大部分本科生都不会。但其实也不难，如果你线性代数功底可以，可以尝试推导矩阵求导公式。）不过这里有一个简便的方法去推断对矩阵求导时应该如何计算：由于这里的参数矩阵本身是 26*289 的，那损失函数对于它的梯度（即损失函数对参数矩阵求导的结果）的尺寸也一定是 26*289 的。而这里每组输入数据的尺寸是 289*1，每组数据对应的部分梯度尺寸为 26*1, 要得到一个 26*289 尺寸的梯度矩阵，就只能是一个 26*1 尺寸的矩阵乘以一个 1*289 尺寸的矩阵，需要对输入数据进行转置。所以这里计算的是`np.dot(dd,xx.T)`。
对一个 batch 里的数据分别求得梯度之后，按照`随机梯度下降算法`的要求，我们需要对所有梯度求平均值，得到 self.dw, 其尺寸为 26*289，刚好与我们的 self.weights 匹配。

由于全连接层对 bias 的部分导数为 1，所以这里对于 bias 的梯度 self.bias 就直接等于从之前的层反向传回来的梯度的平均值。
损失函数对于输入 x 的梯度值 self.dx 的求解与 self.dw 类似。由于输入数据 self.x 中的一个数据的尺寸为 289*1，self.weights 的尺寸为 26*289, dd 的尺寸为 26*1, 所以需要对 self.weights 进行转置。即 “289*1=(289*26)*(26*1)”。

最后是使用梯度更新参数，注意这里的 self.lr 即为前面我们提到过的学习速率`alpha`，它是一个需要我们手工设定的超参数。

这里的矩阵求导确实不太好处理，容易出错，请你仔细分析每一个变量代表的含义，如果对一个地方不清楚，请回到前面看看相关的概念是如何定义的。

## 激活函数层


```python
class Sigmoid:
    def __init__(self):  # 无参数，不需初始化
        pass

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def forward(self, x):
        self.x = x
        self.y = self.sigmoid(x)
        return self.y

    def backward(self, d):
        sig = self.sigmoid(self.x)
        self.dx = d * sig * (1 - sig)
        return self.dx  # 反向传递梯度
```

sigmoid 函数将输出限制在 0 到 1 之间，刚好可以作为概率看待。这里我们有 26 个输入节点，经过 sigmoid 层计算之后，哪个输出节点的数值最大，就认为图片上最有可能是该节点代表的字母。比如如果输出层第 0 个节点值最大，就认为图片上的字母是 “A”, 如果第 25 个节点的值最大，就认为图片上的字母是 “Z”。

注意一般在计算神经网络的深度时我们一般不把激活层算进去，但这里为了编程方便，也将激活函数视为单独的一层。　　

## 损失函数层


```python
class QuadraticLoss:
    def __init__(self):
        pass

    def forward(self, x, label):
        self.x = x
        self.label = np.zeros_like(x)  # 由于我们的label本身只包含一个数字，我们需要将其转换成和模型输出值尺寸相匹配的向量形式
        for a, b in zip(self.label, label):
            a[b] = 1.0  # 只有正确标签所代表的位置概率为1，其他为0
        self.loss = np.sum(np.square(x - self.label)) / self.x.shape[0] / 2  # 求平均后再除以2是为了表示方便
        return self.loss

    def backward(self):
        self.dx = (self.x - self.label) / self.x.shape[0]  # 2被抵消掉了
        return self.dx
```

在`随机梯度下降算法`里，每次前向计算和反向传播都会计算包含多个输入数据的一个 batch。所以损失函数值在随后也要除以 batch 中包含的数据数量,　即`self.x.shape[0]`，同时这里除以了 2,　这个地方的 2 可以和对二次损失函数求导后多出来的系数 2 抵消掉。所以，我们的损失函数变成了：

$$
J(\theta) = \frac{1}{2m} \sum^m_{i=1} (h(\theta,X_i)-Y_i)^2
$$

## 准确率层


```python
class Accuracy:
    def __init__(self):
        pass

    def forward(self, x, label):  # 只需forward
        self.accuracy = np.sum([np.argmax(xx) == ll for xx, ll in zip(x, label)])  # 对预测正确的实例数求和
        self.accuracy = 1.0 * self.accuracy / x.shape[0]
        return self.accuracy
```

如果我们的神经网络的输出层中，概率最大的节点的下标与实际的标签 label 相等，则预测正确。预测正确的数量除以总的数量，就得到了正确率。

## 构建神经网络


```python
def main():
    datalayer1 = Data('train.npy', 1024)  # 用于训练，batch_size设置为1024
    datalayer2 = Data('validate.npy', 10000)  # 用于验证，所以设置batch_size为10000,一次性计算所有的样例
    inner_layers = []
    inner_layers.append(FullyConnect(17 * 17, 26))
    inner_layers.append(Sigmoid())
    losslayer = QuadraticLoss()
    accuracy = Accuracy()

    for layer in inner_layers:
        layer.lr = 1000.0  # 为所有中间层设置学习速率

    epochs = 20
    for i in range(epochs):
        print('epochs:', i)
        losssum = 0
        iters = 0
        while True:
            data, pos = datalayer1.forward()  # 从数据层取出数据
            x, label = data
            for layer in inner_layers:  # 前向计算
                x = layer.forward(x)

            loss = losslayer.forward(x, label)  # 调用损失层forward函数计算损失函数值
            losssum += loss
            iters += 1
            d = losslayer.backward()  # 调用损失层backward函数层计算将要反向传播的梯度

            for layer in inner_layers[::-1]:  # 反向传播
                d = layer.backward(d)

            if pos == 0:  # 一个epoch完成后进行准确率测试
                data, _ = datalayer2.forward()
                x, label = data
                for layer in inner_layers:
                    x = layer.forward(x)
                accu = accuracy.forward(x, label)  # 调用准确率层forward()函数求出准确率
                print('loss:', losssum / iters)
                print('accuracy:', accu)
                break

if __name__ == '__main__':
    main()
```

    epochs: 0
    loss: 0.541326932161
    accuracy: 0.33
    epochs: 1
    loss: 0.322561570007
    accuracy: 0.549
    epochs: 2
    loss: 0.24825038484
    accuracy: 0.5959
    epochs: 3
    loss: 0.214456080778
    accuracy: 0.6824
    epochs: 4
    loss: 0.179479788139
    accuracy: 0.7511
    epochs: 5
    loss: 0.152682180959
    accuracy: 0.7738
    epochs: 6
    loss: 0.134862201922
    accuracy: 0.7989
    epochs: 7
    loss: 0.123318053388
    accuracy: 0.8113
    epochs: 8
    loss: 0.115694421735
    accuracy: 0.8242
    epochs: 9
    loss: 0.110088927924
    accuracy: 0.831
    epochs: 10
    loss: 0.105729115814
    accuracy: 0.8361
    epochs: 11
    loss: 0.102193523711
    accuracy: 0.8394
    epochs: 12
    loss: 0.0992462051966
    accuracy: 0.8528
    epochs: 13
    loss: 0.0967390187206
    accuracy: 0.8541
    epochs: 14
    loss: 0.0945715467217
    accuracy: 0.8566
    epochs: 15
    loss: 0.0926725451782
    accuracy: 0.8584
    epochs: 16
    loss: 0.0909897902294
    accuracy: 0.8596
    epochs: 17
    loss: 0.0894841478902
    accuracy: 0.872
    epochs: 18
    loss: 0.0881259172677
    accuracy: 0.8724
    epochs: 19
    loss: 0.0868923293202
    accuracy: 0.8733



```python

```
