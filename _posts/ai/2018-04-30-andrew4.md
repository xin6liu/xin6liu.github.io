---
layout: post
title: 深度学习
date: 2018-04-30
category: ai
---

- Hyperparameter tuning
- Regularization
- Optimization

- Diagnose price and variants 
- Advance optimization algorithms

上角标代表example的个数，一共有m个

* 从train set error， 看 bias
* 从train set error 和 dev set error 的差，看variance.

L2 regularization

Frobenius norm: the sum of square of elements of a matrix

### “Weight decay”:
以前是w-alpha*dw

现在是(1-alpha*lambda/m)*w-alpha*dw
 
Lambda 很大的话，w会变小，z也会变小。当z变小的时候，每一层都会接近linear.

D3 表示一个三层的dropout 向量：

d3 = np.random.rand(a3.shape[0], a3.shape[1])


## 总结：

dev集的error太大 -> variance太大 -> 需要减少overfitting:

1. Regularization
2. More training data


High bias:

1. Deeper neural network
2. Increase the number of units
3. More test data



## 初始化

如果W和b都初始为零的话，无论几次结果都一样

```python
    for l in range(1, L):
        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
```

如果W太大的话，刚开始cost会很大，运算慢，结果不好

```python
    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*10
        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
```

He initialization: instead of multiplying `np.random.randn(..,..)` by 10, you will multiply it by $\sqrt{\frac{2}{\text{dimension of the previous layer}}}$

```python
    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2/layers_dims[l-1])
        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
```


## Adam optimization algorithm:

循环：
	利用 current mini batch 计算 dw, db 
	新的 Vdw 和 Vdb
	新的 Sdw 和 Sdb
	新的 Vdw^correct 和 Vdb^correct
	新的 Sdw^correct 和 Sdb^correct
	新的 W, b

需要使用 0.9 的加权数之前的数值加上当日温度的 0.1 倍 