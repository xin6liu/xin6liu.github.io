---
layout: post
title: ML
date: 2018-05-05
category: ai
---


## machine learning

Regression 是 supervised learning 中的一种，regression 是预测**连续**的结果(output)。

Classification 是 另外一种监督学习，预测不连续值的输出值(output)。


Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.

Non-clustering: The "Cocktail Party Algorithm", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party).


cost function J (e.g. square error function) = sum of loss function

当J里面有两个theta时，不能先利用gradient descent 更新其中一个，再代入gradient descent 计算另外一个。应该两个都用老的theta计算gradient descent.

Feature scaling: (x - mean) /  range

Convergence: 一次 iteration 之后下降小于 10e-3


## WEEK3：分类问题
参数 theta 决定了decision boundary

Linear regression 问题：
h(x): theta' * x
分类 （Logistic regression）问题：
h(x): sigmoid 方程, output代表的是 p(y=1|x;theta) 也就是y=1的概率。

但是两者的dJ/dtheta都一样：

$$
\theta_j := \theta_j - \alpha \sum_{I=1}^m (h(x^i)) - y^i) x_j^i
$$

### overfiting 
This terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting:

1) Reduce the number of features:
* Manually select which features to keep.
* Use a model selection algorithm (studied later in the course).

2) Regularization
* Keep all the features, but reduce the magnitude of parameters $theta_j$.
* Regularization works well when we have a lot of slightly useful features.


Regularization 的理念就是：

改写cost function。本来cost function 的作用就是用来min的，使theta更拟合training data。在此基础上增加一项，所有theta的和，这样使theta更小。这样一来，就会在（1. 使theta更拟合training data） 和（2.使theta更小）之间达到平衡，trade off。