---
layout: post
title: ML2
date: 2018-05-06
category: ai
---

# 用python实现 Logistic Regression
* 原练习中主文件：ex2.m
* 需要完成的文件：    
    1. sigmoid.m
    2. costFunction.m
    3. predict.m
    4. costFunctionReg.m

## matlab文件ex2.m
```
clear ; close all; clc
 
%% Load Data 两个特征，一个label
data = load('ex2data1.txt');
X = data(:, [1, 2]); y = data(:, 3);

%% ==================== Part 1: Plotting ====================

fprintf(['Plotting data with + indicating (y = 1) examples and o ' ...
         'indicating (y = 0) examples.\n']);

pos = find(y==1); neg = find(y == 0);
plot(X(pos, 1), X(pos, 2), 'k+','LineWidth', 2, 'MarkerSize', 7);
hold on;
plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y', 'MarkerSize', 7);
hold on;
xlabel('Exam 1 score')
ylabel('Exam 2 score')
legend('Admitted', 'Not admitted')
hold off;
```

## python实现


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```


```python
df = pd.read_csv('ex2data1.txt',names=["X1","X2","Y"])
```


```python
df.head()
```




<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X1</th>
      <th>X2</th>
      <th>Y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>34.623660</td>
      <td>78.024693</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>30.286711</td>
      <td>43.894998</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>35.847409</td>
      <td>72.902198</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>60.182599</td>
      <td>86.308552</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>79.032736</td>
      <td>75.344376</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
data = df.as_matrix()
X = data[:,0:2]
Y = data[:,2]
```


```python
colors =['r' if l==0 else 'b' for l in Y]
plt.scatter(X[:,0],X[:,1],c=colors)
plt.xlabel('Exam 1 score')
plt.ylabel('Exam 2 score')
plt.show()
```

![output_7_0](https://i.imgur.com/lCMIxVp.png)


## matlab文件ex2.m
```
%% ============ Part 2: Compute Cost and Gradient ============
% 完成sigmoid.m，costFunction.m
% cost and gradient for logistic regression

[m, n] = size(X);
X = [ones(m, 1) X];
initial_theta = zeros(n + 1, 1);

[cost, grad] = costFunction(initial_theta, X, y);

fprintf('Cost at initial theta (zeros): %f\n', cost);
fprintf('Expected cost (approx): 0.693\n');
fprintf('Gradient at initial theta (zeros): \n');
fprintf(' %f \n', grad);
fprintf('Expected gradients (approx):\n -0.1000\n -12.0092\n -11.2628\n');

% Compute and display cost and gradient with non-zero theta
test_theta = [-24; 0.2; 0.2];
[cost, grad] = costFunction(test_theta, X, y);

fprintf('\nCost at test theta: %f\n', cost);
fprintf('Expected cost (approx): 0.218\n');
fprintf('Gradient at test theta: \n');
fprintf(' %f \n', grad);
fprintf('Expected gradients (approx):\n 0.043\n 2.566\n 2.647\n');
```

### costFunction(theta, X, y)
```
function [J, grad] = costFunction(theta, X, y)
m = length(y); 
J = 0;
grad = zeros(size(theta));
J =  sum(-y.*log(sigmoid(X*theta)) -(1-y).*log(1-sigmoid(X*theta))) / m;
grad = X'*(sigmoid(X*theta) - y) / m;
end
```
### sigmoid(z)

```
function g = sigmoid(z)
g = zeros(size(z));
g = 1 ./ (1 + exp(-z))
end
```

## python实现


```python
def sigmoid(z):
    return 1/(1+np.exp(-z))
```


```python
def costFunction(theta,X,y):
    m = y.shape[1]
    z = np.dot(theta,X)
    a = sigmoid(z)
    cost = -np.sum(y*np.log(a)+(1-y)*np.log(1-a))/m  
    return cost
```


```python
X = X.T 
Y = Y.reshape(1,-1)
(n,m) = np.shape(X)
X = np.insert(X, 0, values=np.ones(m), axis=0)
initial_theta = np.zeros((1,n+1))
```


```python
cost = costFunction(initial_theta, X, Y)
```


```python
cost
```




    0.6931471805599453



## Regularized logistic regression

    function [J, grad] = costFunctionReg(theta, X, y, lambda)

    m = length(y); 
    J = 0;
    grad = zeros(size(theta));

    z = X*theta
    a = sigmoid(z)
    J = 1/m * (-y' * log(a) - (1 - y') * log(1 - a)) + lambda/2/m*sum(theta(2:end).^2);

    nx = size(theta)
    grad(1,:) = 1/m * (X(:, 1)' * (a - y));
    grad(2:nx, :) = 1/m * (X(:, 2:nx)' * (a - y)) + lambda/m*theta(2:nx, :);

    end

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Od9mobDaEeaCrQqTpeD5ng_4f5e9c71d1aa285c1152ed4262f019c1_Screenshot-2016-11-22-09.31.21.png?expiry=1527724800000&hmac=e_MXtnvWs_a70qb9dOkEhKfEYvBVrtxg0lVK9ukIQ6A)
![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/dfHLC70SEea4MxKdJPaTxA_306de28804a7467f7d84da0fe3ee9c7b_Screen-Shot-2016-12-07-at-10.49.02-PM.png?expiry=1527724800000&hmac=PedI8KYSSByY85sujmGEXxguTwimpLCV2mLAkTW96Yw)


```python

```
