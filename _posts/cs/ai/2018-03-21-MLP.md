---
layout: post
section-type: post
title: 反向传播算法
category: tech
tags: [ 'tutorial' ]
---
<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>

* 刘鑫
* 2018.03.21

**多层感知器**（Multilayer Perceptron,缩写MLP）是一种最常见的前向结构的人工神经网络(前馈神经网络)。

**反向传播算法**的监督学习方法常被用来训练MLP, 下面是反向传播算法的笔记。

> 传送门：
>
> [numpy, pandas, seaborn, sklearn 基础](http://liuxin21.com/python,%20cs/2018/03/12/sklearn.html)
> 
> [神经网络模型分类](http://ibillxia.github.io/blog/2013/03/24/classes-of-neural-networks/)
>
> [神经网络基础](http://liuxin21.com/ai/2018/03/18/ai.html)



在之前的文章中提到了神经网络`model`, 衡量`model`性能的损失函数$J(\theta)$, 使$J(\theta)$减小的学习算法`learn`(梯度下降算法)。

这一次我们来探讨**梯度下降**这个`learn`中，如何求$J(\theta)$的梯度。

我们用到的就是——**反向传播算法（backpropogate algorithm）**。

## 反向传播算法 和 “链式法则”

我们考虑只有两个网络层和一个输出层，第一层（输入层）只有两个节点，第二层只有一个节点的情况：

![Screenshot 2018-03-21 16.43.56](https://i.imgur.com/lPgoVkz.png)

其中，$\theta$ 为 $w_{11}, w_{12}, \mathrm{bias}_1$ 中的一个，$x$ 为
$\left[
\begin{matrix}
a_1 \\
a_2 
\end{matrix}
\right]$
。
根据链式法则：

![Screenshot 2018-03-21 16.44.16](https://i.imgur.com/GFY4Jy7.png)

其中，

![Screenshot 2018-03-21 16.53.44](https://i.imgur.com/MG2BDUZ.png)

举个栗子：

![Screenshot 2018-03-21 16.44.16 copy](https://i.imgur.com/ObTOvDi.png)

## Python 代码实现


```python
import numpy as np
```


```python
class C1:
    
    def __init__(self, lx, ly):
        '''
        这里的输入层(a1,a2)的长度lx=2, 输出层(b1)的长度ly=1
        weight矩阵就是1X2, bias就是1X1 
        '''   
        self.weights = np.random.randn(ly,lx)
        self.bias = np.random.randn(ly)


    def forward(self, x):
        '''
        输入：输入层x(a1,a2)
        返回：输出层y(b1)
        '''
        self.x = x
        self.y = np.dot(self.weights, x) + self.bias
        return self.y

    def backward(self, d):
        '''
        输入：导数值d
        返回：参数w的梯度dw
        '''
        self.dw = d * self.x
        # self.db = d
        # self.dx = d * self.weights
        return self.dw
```


上面的代码的三个def完成了三个工作：
1. 随机初始化网络参数w和bias
2. 输入x，计算出这层的y，向前传递给下一层
3. 将前面层向后传递的导数值与这一层的x想乘，得到最后一层对本层参数$\theta$的梯度

然后进行sigmoid层：


```python
class C2:
    def __init__(self):
        pass
    
    def sigmoid(self, x):
        return 1/(1 + np.exp(-x))
    
    def forward(self, x):
        self.x = x
        self.y = self.sigmoid(x)
        return self.y
    
    def backward(self):
        sig = self.sigmoid(self.x)
        self.dx = sig*(1-sig)
        return self.dx
```

把上面两层拼起来，就完成了整体的网络结构：


```python
def main():
    c1 = C1(2,1)
    c2 = C2()
    x = np.array([[1],[2]])
    print(c1.weights, c1.bias, x)
    
    y1 = c1.forward(x)
    y2 = c2.forward(y1)
    print(y1, y2)
    
    dh = c2.backward()
    dw = c1.backward(dh)
    print(dh, dw)
    
if __name__ == '__main__':
    main()
```
```python
    [[-1.07906824 -0.42675882]] [ 0.2913231] [[1]
     [2]]
    [[-1.64126277]] [[ 0.16229331]]
    [[ 0.13595419]] [[ 0.13595419]
     [ 0.27190838]]
```