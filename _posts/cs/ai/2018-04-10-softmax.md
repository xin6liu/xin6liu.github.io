---
layout: post
title: softmax算法
date: 2018-04-10
category: ai
---

softmax算法可以算得上是分类问题的标配了。

激励函数的输出值只能有两个(0,1)或(-1,1)或(0,x)

可以理解为softmax是sigmoid之类的激励函数的拓展。

```python
x = tf.placeholder(‘float’, [None, 输入特征的数量]
y = tf.placeholder(‘float’, [None, 输出分类的数量]

W = tf.Variable(tf.random_normal([输入特征的数量,输出分类的数量]))
b = tf.Variable(tf.zeros([输出分类的数量]))

pred = tf.nn.softmax(tf.matmul(x,W)+b)
```

这里我们假设输入特征的数量为2，输出分类的数量为3，我们只有1个样本。

所以x:[1x2], y:[1x3], W:[2x3], b:[1x3]

x=[x1,x2]

y=[y1,y2,y3]

W=[[W11,w12,w13],[w21,w22,w23]]

b=[b1,b2,b3]

logtis 

= tf.matmul(x,W)+b 

= [w11x1+w21x2+b1, w12x1+w22x2+b2, w13x1+w23x2+b3]

= [logits1, logits2, logits3]

tf.softmax( [logits1, logits2, logits3])

= [exp(logits1), exp(logits2), exp(logits3)] / (exp(logits1)+exp(logits2)+exp(logits3))

可以看出softmax并不改变logits的形状，logits大的变得相对更大，三个加起来等于1.
