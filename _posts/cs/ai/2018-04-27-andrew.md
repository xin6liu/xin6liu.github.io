---
layout: post
title: 吴恩达课程
date: 2018-04-27
category: ai
---


接下来我们说明一些在余下课程中，需要用到的一些符号。

**符号定义** ：

$x$：表示一个$n_x$维数据，为输入数据，维度为$(n_x,1)$； 

$y​$：表示输出结果，取值为$(0,1)​$；

$(x^{(i)},y^{(i)})$：表示第$i$组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据； 

$X=[x^{(1)},x^{(2)},...,x^{(m)}]$：表示所有的训练数据集的输入值，放在一个 $n_x×m$的矩阵中，其中$m$表示样本数目; 

$Y=[y^{(1)},y^{(2)},...,y^{(m)}]$：对应表示所有训练数据集的输出值，维度为$1×m$。


The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.

- dvar: The derivative of a final output variable with respect to various intermediate quantities.

上角标代表example的个数，一共有m个

do not use "rnak 1 array"

eg: `np.random.randn(5)` -> (5,)

应该用： `np.random.randn(1,5) -> (1,5)

Minimizing the loss corresponds with maximizing logp(y|x).


Normalizing rows 的原因：
gradient descent converges faster after normalization.

For example, if $$x = 
\begin{bmatrix}
    0 & 3 & 4 \\
    2 & 6 & 4 \\
\end{bmatrix}$$ then $$\| x\| = np.linalg.norm(x, axis = 1, keepdims = True) = \begin{bmatrix}
    5 \\
    \sqrt{56} \\
\end{bmatrix} $$and        $$ x\_normalized = \frac{x}{\| x\|} = \begin{bmatrix}
    0 & \frac{3}{5} & \frac{4}{5} \\
    \frac{2}{\sqrt{56}} & \frac{6}{\sqrt{56}} & \frac{4}{\sqrt{56}} \\
\end{bmatrix}$$ Note that you can divide matrices of different sizes and it works fine: this is called broadcasting.


sigmoid 函数和 tanh 函数两者共同的缺点是，在𝑧特别大或者特别小的情况下，导数的 梯度或者函数的斜率会变得特别小，最后就会接近于 0，导致降低梯度下降的速度。

修正线性单元的函数(ReLu), z 等于 0 的时候，假设一个导数是 1 或者 0 效果都可以

如果输出是 0、1 值(二分类问题)，则输出层选择 sigmoid 函数，然后其它的所有单 元都选择 Relu 函数。

ReLu 优点：
1. 在𝑧的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中， 使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。
2. sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥而 Relu 和 Leaky ReLu 函数大于 0 部分都为常熟，不会产生梯度弥散现象。(同时应该注 意到的是，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性， 而 Leaky ReLu 不会有这问题)。𝑧在 ReLu 的梯度一半都是 0，但是，有足够的隐藏层使得 z 值大于 0，所以对大多数的 训练数据来说学习过程仍然可以很快。

如果𝑦 是个实数，那么在输出层用线性激活函数也许可行，你的输出也是一个 实数，从负无穷到正无穷。

房价都是非负数，所以我们也可以在输出层使用 ReLU 函数这样你的y都大于等于 0。

我们通常 倾向于初始化为很小的随机数。因为如果你用 tanh 或者 sigmoid 激活函数，或者说只在输 出层有一个 Sigmoid，如果(数值)波动太大，当你计算激活值时𝑧[1] = 𝑊[1]𝑥 + 𝑏[1] , 𝑎[1] = 𝜎(𝑧[1]) = 𝑔[1](𝑧[1])如果𝑊很大，𝑧就会很大。𝑧的一些值𝑎就会很大或者很小，因此这种情况 下你很可能停在 tanh/sigmoid 函数的平坦的地方(见图 3.8.2)，这些地方梯度很小也就意味着 梯度下降会很慢，因此学习也就很慢。
