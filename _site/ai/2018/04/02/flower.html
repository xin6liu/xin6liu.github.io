<!DOCTYPE html>

<html>

<head>
	<title>LiuXin</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/assets/css/main_new.css">
	<link rel="stylesheet" href="/assets/css/syntax.css">
	<!--[if lte IE 9]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie8.css" /><![endif]-->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script>
	MathJax = {
	  tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
	};
	</script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>


<body>

<!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header"	
					>
	<a href="http://localhost:4000/" class="logo"><strong>LiuXin</strong> <span>stay hungry, stay foolish</span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
</header>

<!-- Menu -->
<nav id="menu">
	<ul class="links">
		
		
			
				<li><a href="/home/1python.html">网页开发 Web Development</a></li>
			
		
			
				<li><a href="/home/2ai.html">机器学习 Machine Learning</a></li>
			
		
			
				<li><a href="/home/3cfd.html">计算流体力学 CFD</a></li>
			
		
			
				<li><a href="/home/4cs.html">计算机科学 Computer Science</a></li>
			
		
			
				<li><a href="/home/5mns.html">纳米科学 Nanoscience</a></li>
			
		
			
				<li><a href="/home/6course.html">课程 Course</a></li>
			
		
			
				<li><a href="/home/7diary.html">生活 Diary</a></li>
			
		
			
				<li><a href="/home/8categories.html">标签 tags</a></li>
			
		
	</ul>
	<!-- <ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul> -->
</nav> 
    
    
<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
<!-- title -->
			<h1>使用决策树完成鸢尾花分类</h1>
<!-- 日期 -->		
		
		2018-04-02
		
		</header>
<!-- description -->	
		<h5></h5>
<!-- 图片 -->		
		<!-- 
		 -->
<!-- 正文 -->
		<p><h2 id="一实验介绍">一、实验介绍</h2>

<h3 id="11-实验内容">1.1 实验内容</h3>

<p>决策树是机器学习中一种简单而又经典的算法。本次实验将带领了解决策树的基本原理，并学习使用 scikit-learn 来构建一个决策树分类模型，最后使用此模型预测鸢尾花的种类。</p>

<h3 id="12-实验知识点">1.2 实验知识点</h3>

<ul>
  <li>决策树的基本原理。</li>
  <li>决策树在生成和修剪中使用的 ID3, C4.5 及 CART 算法。</li>
  <li>使用 scikit-learn 中提供的决策树分类器进行实例验证。</li>
</ul>

<h3 id="13-实验环境">1.3 实验环境</h3>

<ul>
  <li>python3</li>
</ul>

<h3 id="14-适合人群">1.4 适合人群</h3>

<p>本课程难度为一般，属于初级级别课程，适合具有 Python 基础，并对机器学习中决策树算法感兴趣的用户。</p>
<h2 id="二决策树基本原理">二、决策树基本原理</h2>

<h3 id="21-决策树简介">2.1 决策树简介</h3>

<p>决策树是一种特殊的树形结构，一般由节点和有向边组成。其中，节点表示特征、属性或者一个类。而有向边包含有判断条件。如图所示，决策树从根节点开始延伸，经过不同的判断条件后，到达不同的子节点。而上层子节点又可以作为父节点被进一步划分为下层子节点。一般情况下，我们从根节点输入数据，经过多次判断后，这些数据就会被分为不同的类别。这就构成了一颗简单的分类决策树。</p>

<p><img src="https://dn-anything-about-doc.qbox.me/document-uid214893labid3141timestamp1498798518976.png/wm" alt="" /></p>

<h3 id="22-决策树学习">2.2 决策树学习</h3>

<p>我们将决策数的思想引入到机器学习中，就产生了一种简单而又经典的预测方法 —— 决策树学习（Decision Tree Learning），亦简称为决策树。决策树可以用来解决分类或回归问题，分别称之为分类树或回归树。其中，分类树的输出是一个标量，而回归树的一般输出为一个实数。</p>

<p>通常情况下，决策树利用损失函数最小的原则建立模型，然后再利用该模型进行预测。决策树学习通常包含三个阶段：特征选择、树的生成，树的修剪。</p>

<h3 id="23-特征选择">2.3 特征选择</h3>

<p>特征选择是建立决策树之前十分重要的一步。如果是随机地选择特征，那么所建立决策树的学习效率将会大打折扣。举例来讲，银行采用决策树来解决信用卡审批问题，判断是否向某人发放信用卡可以根据其年龄、工作单位、是否有不动产、历史信贷情况等特征决定。而选择不同的特征，后续生成的决策树就会不一致，这种不一致最终会影响到决策树的分类效率。</p>

<p>通常我们在选择特征时，会考虑到两种不同的指标，分别为：信息增益和信息增益比。要想弄清楚这两个概念，我们就不得不提到信息论中的另一个十分常见的名词 —— 熵。</p>

<p>熵（Entropy）是表示随机变量不确定性的度量。简单来讲，熵越大，随机变量的不确定性就越大。而特征 A 对于某一训练集 D 的<strong>信息增益 g(D, A) 定义</strong>为集合 D 的熵 H(D) 与特征 A 在给定条件下 D 的熵 H(D/A) 之差。</p>

<p><img src="https://dn-anything-about-doc.qbox.me/document-uid214893labid3141timestamp1498793388103.png/wm" alt="" /></p>

<p>上面这段定义读起来很拗口，也不是特别容易理解。那么，下面我使用更通俗的语言概述一下。简单来讲，每一个特征针对训练数据集的前后信息变化的影响是不一样的，信息增益越大，即代表这种影响越大。而影响越大，就表明该特征更加重要。</p>

<h3 id="24-生成算法">2.4 生成算法</h3>

<p>当我们了解信息增益的概念之后，我们就可以学习决策树的生成算法了。其中，最经典的就数 John Ross Quinlan 提出的 ID3 算法，这个算法的核心理论即源于上面提到的信息增益。</p>

<p>ID3 算法通过递归的方式建立决策树。建立时，从根节点开始，对节点计算每个独立特征的信息增益，选择信息增益最大的特征作为节点特征。接下来，对该特征施加判断条件，建立子节点。然后针对子节点再此使用信息增益进行判断，直到所有特征的信息增益很小或者没有特征时结束，这样就逐步建立一颗完整的决策树。</p>

<p>除了从信息增益演化而来的 ID3 算法，还有一种常见的算法叫 C4.5。C4.5 算法同样由 John Ross Quinlan 发明，但它使用了信息增益比来选择特征，这被看成是 ID3 算法的一种改进。</p>

<p>ID3 和 C4.5 算法简单高效，但是他俩均存在一个缺点，那就是用 “完美去造就了另一个不完美”。这两个算法从信息增益和信息增益比开始，对整个训练集进行的分类，拟合出来的模型针对该训练集的确是非常完美的。但是，这种完美就使得整体模型的复杂度较高，而对其他数据集的预测能力就降低了，也就是我们常说的过拟合而使得模型的泛化能力变弱。</p>

<p>当然，过拟合的问题也是可以解决的，那就是对决策树进行修剪。</p>

<h3 id="25-决策树修剪">2.5 决策树修剪</h3>

<p>决策树的修剪，其实就是通过优化损失函数来去掉不必要的一些分类特征，降低模型的整体复杂度。修剪的方式，就是从树的叶节点出发，向上回缩，逐步判断。如果去掉某一特征后，整棵决策树所对应的损失函数更小，那就就将该特征及带有的分支剪掉。</p>

<p><img src="https://dn-anything-about-doc.qbox.me/document-uid214893labid3141timestamp1498801422852.png/wm" alt="" /></p>

<p>由于 ID3 和 C4.5 只能生成决策树，而修剪需要单独进行，这也就使得过程更加复杂了。1984 年，Breiman 提出了 CART 算法，使这个过程变得可以一步到位。CART 算法本身就包含了决策树的生成和修剪，并且可以同时被运用到分类树和回归树。这就是和 ID3 及 C4.5 之间的最大区别。</p>

<p>CART 算法在生成树的过程中，分类树采用了基尼指数（Gini Index）最小化原则，而回归树选择了平方损失函数最小化原则。基尼指数其实和前面提到的熵的概念是很相似的。简单概述区别的话，就是数值相近但不同，而基尼指数在运算过程中的速度会更快一些。</p>

<p>CART 算法也包含了树的修剪。CART 算法从完全生长的决策树底端剪去一些子树，使得模型更加简单。而修剪这些子树时，是每次去除一颗，逐步修剪直到根节点，从而形成一个子树序列。最后，对该子树序列进行交叉验证，再选出最优的子树作为最终决策树。</p>

<h2 id="三鸢尾花分类实验">三、鸢尾花分类实验</h2>

<p>如果你感觉理论看起来比较费劲，不用担心。接下来就带领你用非常少的代码量来构建一个决策树分类模型，实现对鸢尾花分类。</p>

<h3 id="31-数据集简介">3.1 数据集简介</h3>

<p>鸢尾花数据集是机器学习领域一个非常经典的分类数据集。接下来，我们就用这个训练集为基础，一步一步地训练一个机器学习模型。首先，我们来看一下该数据集的基本构成。数据集名称的准确名称为 <a href="https://archive.ics.uci.edu/ml/datasets/iris">Iris Data Set</a>，总共包含 150 行数据。每一行数据由 4 个特征值及一个目标值组成。其中 4 个特征值分别为：萼片长度、萼片宽度、花瓣长度、花瓣宽度。而目标值及为三种不同类别的鸢尾花，分别为：Iris Setosa，Iris Versicolour，Iris Virginica。</p>

<p><img src="https://dn-anything-about-doc.qbox.me/document-uid214893labid3141timestamp1498790523682.png/wm" alt="" /></p>

<h3 id="32-数据获取及划分">3.2 数据获取及划分</h3>

<p>你可以通过著名的 UCI 机器学习数据集网站下载<a href="https://archive.ics.uci.edu/ml/datasets/iris">该数据集</a>。本实验中，为了更加便捷地实验。我们直接实验 scikit-learn 提供的方法导入该数据集即可。
<strong>☞ 示例代码：</strong></p>
<pre><code class="language-python"># -*-  coding: utf-8 -*-
from sklearn import datasets # 导入方法类

iris = datasets.load_iris() # 加载 iris 数据集
iris_feature = iris.data # 特征数据
iris_target = iris.target # 分类数据

iris_target # 查看 iris_target
</code></pre>
<p><strong>☞ 动手练习：</strong>
接下来，你可以直接通过 <code>print(iris_target)</code> 查看一下花的分类数据。这里，scikit-learn 已经将花的原名称进行了转换，其中 0, 1, 2 分别代表 Iris Setosa, Iris Versicolour 和 Iris Virginica。</p>
<pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
</code></pre>
<p>你会发现，这些数据是按照鸢尾花类别的顺序排列的。所以，如果我们将其直接划分为训练集和数据集的话，就会造成数据的分布不均。详细来讲，直接划分容易造成某种类型的花在训练集中一次都未出现，训练的模型就永远不可能预测出这种花来。你可能会想到，我们将这些数据大乱后再划分训练集和数据集。当然，更方便地，scikit-learn 为我们提供了训练集和数据集的方法。
<strong>☞ 示例代码：</strong></p>
<pre><code class="language-python">from sklearn.model_selection import train_test_split

feature_train, feature_test, target_train, target_test = train_test_split(iris_feature, iris_target, test_size=0.33, random_state=42)

target_train
</code></pre>
<p><strong>☞ 动手练习：</strong>
其中，<code>feature_train</code>, <code>feature_test</code>, <code>target_train</code>, <code>target_test</code> 分别代表训练集特征、测试集特征、训练集目标值、验证集特征。<code>test_size</code> 参数代表划分到测试集数据占全部数据的百分比，你也可以用 <code>train_size</code> 来指定训练集所占全部数据的百分比。一般情况下，我们会将整个训练集划分为 70% 训练集和 30% 测试集。最后的 <code>random_state</code> 参数表示乱序程度。</p>

<p>数据集划分之后，我们可以再次执行 <code>print target_train</code> 看一下结果。</p>

<pre><code>array([1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 0,
       2, 2, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 0, 0, 1, 1, 2, 1, 2,
       2, 1, 0, 0, 2, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1, 1, 2, 1, 2, 0, 2,
       1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 0, 1, 2, 2,
       1, 2, 1, 1, 2, 2, 0, 1, 2, 0, 1, 2])
</code></pre>
<p>现在，你会发现花的种类已经变成了乱序状态，并且只包含有整个训练集的 70% 数据。</p>

<h3 id="32-模型训练及预测">3.2 模型训练及预测</h3>

<p>划分完训练集和测试集之后，我们就可以开始预测了。首先是从 scikit-learn 中导入决策树分类器。然后实验 fit 方法和 predict 方法对模型进行训练和预测。
<strong>☞ 示例代码：</strong></p>
<pre><code class="language-python"># -*-  coding: utf-8 -*-
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier() # 所以参数均置为默认状态
dt_model.fit(feature_train,target_train) # 使用训练集训练模型
predict_results = dt_model.predict(feature_test) # 使用模型对测试集进行预测
</code></pre>
<p><strong>☞ 动手练习：</strong>
<code>DecisionTreeClassifier()</code> 模型方法中也包含非常多的参数值。例如：</p>

<ul>
  <li><code>criterion = gini/entropy</code> 可以用来选择用基尼指数或者熵来做损失函数。</li>
  <li><code>splitter = best/random</code> 用来确定每个节点的分裂策略。支持 “最佳” 或者“随机”。</li>
  <li><code>max_depth = int</code> 用来控制决策树的最大深度，防止模型出现过拟合。</li>
  <li><code>min_samples_leaf = int</code> 用来设置叶节点上的最少样本数量，用于对树进行修剪。</li>
</ul>

<p>我们可以将预测结果和测试集的真实值分别输出，对照比较。
<strong>☞ 示例代码：</strong></p>
<pre><code class="language-python">print('predict_results:', predict_results)
print('target_test:', target_test)
</code></pre>
<p><strong>☞ 动手练习：</strong>
<strong>☞ 参考结果：</strong></p>
<pre><code>predict_results: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1
 0 0 0 2 1 1 0 0 1 1 2 1 2]
target_test: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1
 0 0 0 2 1 1 0 0 1 2 2 1 2]
</code></pre>
<p>当然，我们可以通过 scikit-learn 中提供的评估计算方法查看预测结果的准确度。
<strong>☞ 示例代码：</strong></p>
<pre><code class="language-python">from sklearn.metrics import accuracy_score

print(accuracy_score(predict_results, target_test))
</code></pre>
<p><strong>☞ 动手练习：</strong>
其实，在 scikit-learn 中的分类决策树模型就带有 score 方法，只是传入的参数和 <code>accuracy_score()</code> 不太一致。
<strong>☞ 示例代码：</strong></p>
<pre><code class="language-python">scores = dt_model.score(feature_test, target_test)
scores
</code></pre>
<p><strong>☞ 动手练习：</strong>
你可以看出两种准确度方法输入参数的区别。一般情况下，模型预测的准确度会和多方面因素相关。首先是数据集质量，本实验中，我们使用的数据集非常规范，几乎不包含噪声，所以预测准确度非常高。其次，模型的参数也会对预测结果的准确度造成影响。</p>

<h2 id="六实验总结">六、实验总结</h2>

<p>首先通过决策树的原理，加深了对介绍机器学习中决策树算法的理解。并采用 scikit-learn 中提供的决策树分类器构建预测模型，实现对鸢尾花进行分类。</p>

<h2 id="七课后习题">七、课后习题</h2>

<ol>
  <li>尝试通过修改 <code>DecisionTreeClassifier()</code> 方法里面的值，查看模型参数对实验结果带来的影响。</li>
  <li>尝试载入 scikit-learn 中提供的另一个著名的 digits 数据集，同样实验决策树分类器实现手写字体识别实验。</li>
</ol>

<h2 id="八参考链接">八、参考链接</h2>

<ol>
  <li>《统计学习方法》，李航，清华大学出版社</li>
  <li><a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">鸢尾花数据集</a>，维基百科</li>
  <li><a href="https://py131.github.io/2017/04/05/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E4%B9%A0%E9%A2%98%E8%A7%A3%E7%AD%94/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E4%B9%A0%E9%A2%98%E8%A7%A3%E7%AD%94%EF%BC%9ACh4.4%20-%20%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0CART%E7%AE%97%E6%B3%95%E4%B8%8E%E5%89%AA%E6%9E%9D%E6%93%8D%E4%BD%9C/">周志华《机器学习》习题解答：Ch4.4 - 编程实现 CART 算法与剪枝操作</a></li>
</ol>

<p>版权由实验楼所有</p>
</p>
	</div>

</section>
</div>


<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				
				
				<li><a href="http://weibo.com/u/2377304267" class="icon alt fa-weibo" target="_blank"><span class="label">weibo</span></a></li>
				
				
				<li><a href="https://www.facebook.com/profile.php?id=100009585159226" class="icon alt fa-facebook" target="_blank"><span class="label">Facebook</span></a></li>
				
				
				<li><a href="https://www.instagram.com/harryliuxin/" class="icon alt fa-instagram" target="_blank"><span class="label">Instagram</span></a></li>
				
				
				
				
				
				<li><a href="http://www.miaopai.com/u/wxsso_p4mslofx14" class="icon alt fa-video-camera" target="_blank"><span class="label">video</span></a></li>
				
				
				<li><a href="https://www.linkedin.com/in/xin-liu-868501134" class="icon alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
				
			</ul>
			<ul class="copyright">
				<li>&copy; LiuXin </li>
				<li> stay hungry, stay foolish</li>
				<li>Designed by: <a href="https://liuxin.in/about/" target="_blank">刘鑫</a></li>

			</ul>
		</div>
	</footer>

	

</body>

</html>