<!DOCTYPE html>

<html>

<head>
	<title>LiuXin</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/assets/css/main_new.css">
	<link rel="stylesheet" href="/assets/css/syntax.css">
	<!--[if lte IE 9]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie8.css" /><![endif]-->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script>
	MathJax = {
		options: {
			// processHtmlClass: 'math-tex',
			// ignoreHtmlClass: '.*',
			renderActions: {
				/* add a new named action not to override the original 'find' action */
				find_script_mathtex: [10, function (doc) {
					for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
					const display = !!node.type.match(/; *mode=display/);
					const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
					const text = document.createTextNode('');
					node.parentNode.replaceChild(text, node);
					math.start = {node: text, delim: '', n: 0};
					math.end = {node: text, delim: '', n: 0};
					doc.math.push(math);
					}
				}, '']
			}
		},
		tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
	};
	</script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>


<body>

<!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header"	
					>
	<a href="http://localhost:4000/" class="logo"><strong>LiuXin</strong> <span>stay hungry, stay foolish</span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
</header>

<!-- Menu -->
<nav id="menu">
	<ul class="links">
		
		
			
				<li><a href="/home/1python.html">网页开发 Web Development</a></li>
			
		
			
				<li><a href="/home/2ai.html">机器学习 Machine Learning</a></li>
			
		
			
				<li><a href="/home/3cfd.html">计算流体力学 CFD</a></li>
			
		
			
				<li><a href="/home/4cs.html">计算机科学 Computer Science</a></li>
			
		
			
				<li><a href="/home/5mns.html">纳米科学 Nanoscience</a></li>
			
		
			
				<li><a href="/home/6course.html">课程 Course</a></li>
			
		
			
				<li><a href="/home/7diary.html">生活 Diary</a></li>
			
		
			
				<li><a href="/home/8categories.html">标签 tags</a></li>
			
		
	</ul>
	<!-- <ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul> -->
</nav> 
    
    
<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
<!-- title -->
			<h1>吴恩达课程</h1>
<!-- 日期 -->		
		
		2018-04-27
		
		</header>
<!-- description -->	
		<h5></h5>
<!-- 图片 -->		
		<!-- 
		 -->
<!-- 正文 -->
		<p><p>接下来我们说明一些在余下课程中，需要用到的一些符号。</p>

<p><strong>符号定义</strong> ：</p>

<p>$x$：表示一个$n_x$维数据，为输入数据，维度为$(n_x,1)$；</p>

<p>$y​$：表示输出结果，取值为$(0,1)​$；</p>

<p>$(x^{(i)},y^{(i)})$：表示第$i$组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据；</p>

<p>$X=[x^{(1)},x^{(2)},…,x^{(m)}]$：表示所有的训练数据集的输入值，放在一个 $n_x×m$的矩阵中，其中$m$表示样本数目;</p>

<p>$Y=[y^{(1)},y^{(2)},…,y^{(m)}]$：对应表示所有训练数据集的输出值，维度为$1×m$。</p>

<p>The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.</p>

<ul>
  <li>dvar: The derivative of a final output variable with respect to various intermediate quantities.</li>
</ul>

<p>上角标代表example的个数，一共有m个</p>

<p>do not use “rnak 1 array”</p>

<p>eg: <code>np.random.randn(5)</code> -&gt; (5,)</p>

<p>应该用： `np.random.randn(1,5) -&gt; (1,5)</p>

<table>
  <tbody>
    <tr>
      <td>Minimizing the loss corresponds with maximizing logp(y</td>
      <td>x).</td>
    </tr>
  </tbody>
</table>

<p>Normalizing rows 的原因：
gradient descent converges faster after normalization.</p>

<p>For example, if <script type="math/tex">% <![CDATA[
x = 
\begin{bmatrix}
    0 & 3 & 4 \\
    2 & 6 & 4 \\
\end{bmatrix} %]]></script> then <script type="math/tex">\| x\| = np.linalg.norm(x, axis = 1, keepdims = True) = \begin{bmatrix}
    5 \\
    \sqrt{56} \\
\end{bmatrix}</script>and        <script type="math/tex">% <![CDATA[
x\_normalized = \frac{x}{\| x\|} = \begin{bmatrix}
    0 & \frac{3}{5} & \frac{4}{5} \\
    \frac{2}{\sqrt{56}} & \frac{6}{\sqrt{56}} & \frac{4}{\sqrt{56}} \\
\end{bmatrix} %]]></script> Note that you can divide matrices of different sizes and it works fine: this is called broadcasting.</p>

<p>sigmoid 函数和 tanh 函数两者共同的缺点是，在𝑧特别大或者特别小的情况下，导数的 梯度或者函数的斜率会变得特别小，最后就会接近于 0，导致降低梯度下降的速度。</p>

<p>修正线性单元的函数(ReLu), z 等于 0 的时候，假设一个导数是 1 或者 0 效果都可以</p>

<p>如果输出是 0、1 值(二分类问题)，则输出层选择 sigmoid 函数，然后其它的所有单 元都选择 Relu 函数。</p>

<p>ReLu 优点：</p>
<ol>
  <li>在𝑧的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中， 使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。</li>
  <li>sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥而 Relu 和 Leaky ReLu 函数大于 0 部分都为常熟，不会产生梯度弥散现象。(同时应该注 意到的是，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性， 而 Leaky ReLu 不会有这问题)。𝑧在 ReLu 的梯度一半都是 0，但是，有足够的隐藏层使得 z 值大于 0，所以对大多数的 训练数据来说学习过程仍然可以很快。</li>
</ol>

<p>如果𝑦 是个实数，那么在输出层用线性激活函数也许可行，你的输出也是一个 实数，从负无穷到正无穷。</p>

<p>房价都是非负数，所以我们也可以在输出层使用 ReLU 函数这样你的y都大于等于 0。</p>

<p>我们通常 倾向于初始化为很小的随机数。因为如果你用 tanh 或者 sigmoid 激活函数，或者说只在输 出层有一个 Sigmoid，如果(数值)波动太大，当你计算激活值时𝑧[1] = 𝑊[1]𝑥 + 𝑏[1] , 𝑎[1] = 𝜎(𝑧[1]) = 𝑔<a href="𝑧[1]">1</a>如果𝑊很大，𝑧就会很大。𝑧的一些值𝑎就会很大或者很小，因此这种情况 下你很可能停在 tanh/sigmoid 函数的平坦的地方(见图 3.8.2)，这些地方梯度很小也就意味着 梯度下降会很慢，因此学习也就很慢。</p>
</p>
	</div>

</section>
</div>


<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				
				
				<li><a href="http://weibo.com/u/2377304267" class="icon alt fa-weibo" target="_blank"><span class="label">weibo</span></a></li>
				
				
				<li><a href="https://www.facebook.com/profile.php?id=100009585159226" class="icon alt fa-facebook" target="_blank"><span class="label">Facebook</span></a></li>
				
				
				<li><a href="https://www.instagram.com/harryliuxin/" class="icon alt fa-instagram" target="_blank"><span class="label">Instagram</span></a></li>
				
				
				
				
				
				<li><a href="http://www.miaopai.com/u/wxsso_p4mslofx14" class="icon alt fa-video-camera" target="_blank"><span class="label">video</span></a></li>
				
				
				<li><a href="https://www.linkedin.com/in/xin-liu-868501134" class="icon alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
				
			</ul>
			<ul class="copyright">
				<li>&copy; LiuXin </li>
				<li> stay hungry, stay foolish</li>
				<li>Designed by: <a href="https://liuxin.in/about/" target="_blank">刘鑫</a></li>

			</ul>
		</div>
	</footer>

	

</body>

</html>