<!DOCTYPE html>

<html>

<head>
	<title>LiuXin</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/assets/css/main_new.css">
	<link rel="stylesheet" href="/assets/css/syntax.css">
	<!--[if lte IE 9]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie8.css" /><![endif]-->
	<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>


<body>

<!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header"	
					>
	<a href="http://localhost:4000/" class="logo"><strong>LiuXin</strong> <span>stay hungry, stay foolish</span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
</header>

<!-- Menu -->
<nav id="menu">
	<ul class="links">
		<!-- 
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		 -->
        <!-- 
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000/">Home</a></li>
	    	
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		 -->
		
		
        
		<li><a href="http://localhost:4000/">网页开发 Web Development</a></li>
        
		
        
		<li><a href="http://localhost:4000/">机器学习 Machine Learning</a></li>
        
		
        
		<li><a href="http://localhost:4000/">计算流体力学 CFD</a></li>
        
		
        
		<li><a href="http://localhost:4000/">计算机科学 Computer Science</a></li>
        
		
        
		<li><a href="http://localhost:4000/">纳米科学 Nanoscience</a></li>
        
		
        
		<li><a href="http://localhost:4000/">课程 Course</a></li>
        
		
        
		<li><a href="http://localhost:4000/">生活 Diary</a></li>
        
		
        
		<li><a href="http://localhost:4000/">总目录 Categories</a></li>
        
		
		

		<!-- 
		    
		        <li><a href="http://localhost:4000/home/1python.html">网页开发 Web Development</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/2ai.html">机器学习 Machine Learning</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/3cfd.html">计算流体力学 CFD</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/4cs.html">计算机科学 Computer Science</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/5mns.html">纳米科学 Nanoscience</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/6course.html">课程 Course</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/7diary.html">生活 Diary</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/8categories.html">总目录 Categories</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/file/3cfd/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/PDE">PDE</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/PDE/PDE%E5%9F%BA%E7%A1%80">PDE基础</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/file/about.html">我的故事 My Stories</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/file/category.html">目录 Category</a></li>
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000/file/3cfd/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E5%8F%98%E5%88%86%E6%B3%95">变分法</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/file/3cfd/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E6%9C%89%E9%99%90%E4%BD%93%E7%A7%AF%E6%B3%95">有限体积法</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E6%9C%89%E9%99%90%E5%85%83%E6%B3%95">有限元法</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6">流体力学</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E6%9C%89%E9%99%90%E4%BD%93%E7%A7%AF%E6%B3%95/%E7%A6%BB%E6%95%A3%E6%96%B9%E6%B3%95">离散方法</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E8%BF%9E%E7%BB%AD%E4%BB%8B%E8%B4%A8%E5%8A%9B%E5%AD%A6">连续介质力学</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E6%9C%89%E9%99%90%E4%BD%93%E7%A7%AF%E6%B3%95/%E9%80%9F%E5%BA%A6%E5%8E%8B%E5%8A%9B%E8%80%A6%E5%90%88%E6%96%B9%E6%B3%95">速度压力耦合方法</a></li>
		    
		 -->
	</ul>
	<!-- <ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul> -->
</nav> 
    
    
<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
<!-- title -->
			<h1>2-layer Neural Network</h1>
<!-- 日期 -->		
		
		2018-04-27
		
		</header>
<!-- description -->	
		<h5></h5>
<!-- 图片 -->		
		<!-- 
		 -->
<!-- 正文 -->
		<p><pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from classes.data import Data
%matplotlib inline
np.random.seed(1)
</code></pre>

<pre><code class="language-python">X = Data.X()
Y = Data.Y()
</code></pre>

<pre><code class="language-python">plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral)
</code></pre>

<pre><code>&lt;matplotlib.collections.PathCollection at 0x10dddf240&gt;
</code></pre>

<p><img src="https://i.imgur.com/ppiRdks.png" alt="output_2_1" /></p>

<pre><code class="language-python">shape_X = np.shape(X)
shape_Y = np.shape(Y)
m = shape_X[1]
print(shape_X,shape_Y,m)
</code></pre>

<pre><code>(2, 400) (1, 400) 400
</code></pre>

<h2 id="neural-network-model">Neural Network model</h2>

<p><strong>Here is our model</strong>:
<img src="https://i.imgur.com/IpQUh85.png" alt="classification_kiank" /></p>

<p><strong>Mathematically</strong>:</p>

<p>For one example $x^{(i)}$:
<script type="math/tex">z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\tag{1}</script> 
<script type="math/tex">a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2}</script>
<script type="math/tex">z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\tag{3}</script>
<script type="math/tex">\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}</script>
<script type="math/tex">% <![CDATA[
y^{(i)}_{prediction} = \begin{cases} 1 & \mbox{if } a^{[2](i)} > 0.5 \\ 0 & \mbox{otherwise } \end{cases}\tag{5} %]]></script></p>

<p>Given the predictions on all the examples, you can also compute the cost $J$ as follows: 
<script type="math/tex">J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small \tag{6}</script></p>

<ol>
  <li>Define the neural network structure ( # of input units,  # of hidden units, etc).</li>
  <li>Initialize the model’s parameters</li>
  <li>Loop:
    <ul>
      <li>Implement forward propagation</li>
      <li>Compute loss</li>
      <li>Implement backward propagation to get the gradients</li>
      <li>Update parameters (gradient descent)</li>
    </ul>
  </li>
</ol>

<p>You often build helper functions to compute steps 1-3 and then merge them into one function we call <code>nn_model()</code>. Once you’ve built <code>nn_model()</code> and learnt the right parameters, you can make predictions on new data.</p>

<pre><code class="language-python">def layer_sizes(X, Y):
    n_x = np.shape(X)[0] # size of input layer
    n_h = 4
    n_y = np.shape(Y)[0] # size of output layer
    return (n_x, n_h, n_y)
</code></pre>

<pre><code class="language-python">def initialize_parameters(n_x, n_h, n_y):
    np.random.seed(2) 
    W1 = np.random.randn(n_h,n_x)
    b1 = np.zeros((n_h,1))
    W2 = np.random.randn(n_y,n_h)
    b2 = np.zeros((n_y,1))  
    parameters = {"W1": W1,"b1": b1,"W2": W2,"b2": b2}
    return parameters
</code></pre>

<pre><code class="language-python">def sigmoid(z):
    return 1/(1+np.exp(-z))

def forward_propagation(X, parameters):
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]

    Z1 = np.dot(W1,X)+b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2,A1)+b2
    A2 = sigmoid(Z2)
    
    cache = {"Z1": Z1,"A1": A1,"Z2": Z2,"A2": A2}
    return A2, cache
</code></pre>

<pre><code class="language-python">def compute_cost(A2, Y, parameters):
    m = Y.shape[1] # number of example
    logprobs = np.multiply(np.log(A2),Y)+np.multiply(np.log(1-A2),(1-Y))
    cost = -np.sum(logprobs)/m
    cost = np.squeeze(cost)     # E.g., turns [[17]] into 17     
    return cost
</code></pre>

<p>Use the six equations on the right of this slide, since you are building a vectorized implementation.</p>

<p><img src="https://i.imgur.com/8S18nKN.png" alt="grad_summary" /></p>

<ul>
  <li>Note that $*$ denotes elementwise multiplication.</li>
</ul>

<p>To compute dZ1 you’ll need to compute $g^{[1]’}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]’}(z) = 1-a^2$. So you can compute 
    $g^{[1]’}(Z^{[1]})$ using <code>(1 - np.power(A1, 2))</code>.</p>

<pre><code class="language-python">def backward_propagation(parameters, cache, X, Y):
    m = X.shape[1]
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    A1 = cache["A1"]
    A2 = cache["A2"]

    dZ2 = A2 - Y
    dW2 = np.dot(dZ2,A1.T)/m
    db2 = np.sum(dZ2,axis=1,keepdims=True)/m
    dZ1 = np.dot(W2.T,dZ2)*(1-np.power(A1,2))
    dW1 = np.dot(dZ1,X.T)/m
    db1 = np.sum(dZ1,axis=1,keepdims=True)/m
    
    grads = {"dW1": dW1,"db1": db1,"dW2": dW2,"db2": db2}
    
    return grads
</code></pre>

<pre><code class="language-python">def update_parameters(parameters, grads, learning_rate = 1.2):
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    dW1 = grads["dW1"]
    db1 = grads["db1"]
    dW2 = grads["dW2"]
    db2 = grads["db2"]

    W1 = W1 - learning_rate*dW1
    b1 = b1 - learning_rate*db1
    W2 = W2 - learning_rate*dW2
    b2 = b2 - learning_rate*db2

    parameters = {"W1": W1,"b1": b1,"W2": W2,"b2": b2}
    
    return parameters
</code></pre>

<pre><code class="language-python">def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]

    for i in range(0, num_iterations):
        A2, cache = forward_propagation(X, parameters)
        cost = compute_cost(A2, Y, parameters)
        grads = backward_propagation(parameters, cache, X, Y)
        parameters = update_parameters(parameters, grads)

        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    return parameters
</code></pre>

<pre><code class="language-python">def predict(parameters, X):
    A2, cache = forward_propagation(X, parameters)
    predictions = (A2 &gt; 0.5)
    return predictions
</code></pre>

<pre><code class="language-python">parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)

# Plot the decision boundary
#plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
#plt.title("Decision Boundary for hidden layer size " + str(4))
</code></pre>
</p>
	</div>

</section>
</div>


<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				
				
				<li><a href="http://weibo.com/u/2377304267" class="icon alt fa-weibo" target="_blank"><span class="label">weibo</span></a></li>
				
				
				<li><a href="https://www.facebook.com/profile.php?id=100009585159226" class="icon alt fa-facebook" target="_blank"><span class="label">Facebook</span></a></li>
				
				
				<li><a href="https://www.instagram.com/harryliuxin/" class="icon alt fa-instagram" target="_blank"><span class="label">Instagram</span></a></li>
				
				
				
				
				
				<li><a href="http://www.miaopai.com/u/wxsso_p4mslofx14" class="icon alt fa-video-camera" target="_blank"><span class="label">video</span></a></li>
				
				
				<li><a href="https://www.linkedin.com/in/xin-liu-868501134" class="icon alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
				
			</ul>
			<ul class="copyright">
				<li>&copy; LiuXin </li>
				<li> stay hungry, stay foolish</li>
				<li>Designed by: <a href="https://liuxin.in/about/" target="_blank">刘鑫</a></li>

			</ul>
		</div>
	</footer>

	

</body>

</html>