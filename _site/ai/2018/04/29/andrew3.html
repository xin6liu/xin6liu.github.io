<!DOCTYPE html>

<html>

<head>
	<title>LiuXin</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/assets/css/main_new.css">
	<link rel="stylesheet" href="/assets/css/syntax.css">
	<!--[if lte IE 9]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie8.css" /><![endif]-->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script>
	MathJax = {
	  tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
	};
	</script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>


<body>

<!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header"	
					>
	<a href="http://localhost:4000/" class="logo"><strong>LiuXin</strong> <span>stay hungry, stay foolish</span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
</header>

<!-- Menu -->
<nav id="menu">
	<ul class="links">
		
		
			
				<li><a href="/home/1python.html">网页开发 Web Development</a></li>
			
		
			
				<li><a href="/home/2ai.html">机器学习 Machine Learning</a></li>
			
		
			
				<li><a href="/home/3cfd.html">计算流体力学 CFD</a></li>
			
		
			
				<li><a href="/home/4cs.html">计算机科学 Computer Science</a></li>
			
		
			
				<li><a href="/home/5mns.html">纳米科学 Nanoscience</a></li>
			
		
			
				<li><a href="/home/6course.html">课程 Course</a></li>
			
		
			
				<li><a href="/home/7diary.html">生活 Diary</a></li>
			
		
			
				<li><a href="/home/8categories.html">标签 tags</a></li>
			
		
	</ul>
	<!-- <ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul> -->
</nav> 
    
    
<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
<!-- title -->
			<h1>Deep Neural Network</h1>
<!-- 日期 -->		
		
		2018-04-29
		
		</header>
<!-- description -->	
		<h5></h5>
<!-- 图片 -->		
		<!-- 
		 -->
<!-- 正文 -->
		<p><p>上一篇文章你可能学会了training a 2-layer Neural Network (with a single hidden layer)，这一篇文章我们建立一个deep neural network, with as many layers as you want.</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
np.random.seed(1)
</code></pre>

<pre><code class="language-python">def initialize_parameters(n_x, n_h, n_y):
    np.random.seed(1)
    W1 = np.random.randn(n_h,n_x)*0.01
    b1 = np.zeros((n_h,1))
    W2 = np.random.randn(n_y,n_h)*0.01
    b2 = np.zeros((n_y,1))    
    parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2}    
    return parameters    
</code></pre>

<pre><code class="language-python">def initialize_parameters_deep(layer_dims):
    np.random.seed(3)
    parameters = {}
    L = len(layer_dims)            # number of layers in the network

    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01
        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))
        
    return parameters
</code></pre>

<pre><code class="language-python">def linear_forward(A, W, b):
    Z = np.dot(W,A)+b
    cache = (A, W, b)  
    return Z, cache
</code></pre>

<pre><code class="language-python">def linear_activation_forward(A_prev, W, b, activation):
    if activation == "sigmoid":
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = sigmoid(Z)
    elif activation == "relu":
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = relu(Z)
    cache = (linear_cache, activation_cache)
    return A, cache
</code></pre>

<pre><code class="language-python">def L_model_forward(X, parameters):
    caches = []
    A = X
    L = len(parameters) // 2                  
    for l in range(1, L):
        A_prev = A 
        A, cache = linear_activation_forward(A_prev, parameters["W"+str(l)], parameters["b"+str(l)], "relu")
        caches.append(cache)
    AL, cache = linear_activation_forward(A, parameters["W"+str(L)], parameters["b"+str(L)], "sigmoid")
    caches.append(cache)        
    return AL, caches
</code></pre>

<pre><code class="language-python">def compute_cost(AL, Y):
    m = Y.shape[1]
    cost = -np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL),axis=1,keepdims=True)/m
    cost = np.squeeze(cost)        
    return cost
</code></pre>

<pre><code class="language-python">def linear_backward(dZ, cache):
    A_prev, W, b = cache
    m = A_prev.shape[1]
    dW = np.dot(dZ,A_prev.T)/m
    db = np.sum(dZ,axis=1,keepdims=True)/m
    dA_prev = np.dot(W.T,dZ)
    return dA_prev, dW, db
</code></pre>

<pre><code class="language-python">def linear_activation_backward(dA, cache, activation):
    linear_cache, activation_cache = cache    
    if activation == "relu":
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)        
    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
    return dA_prev, dW, db
</code></pre>

<pre><code class="language-python">def L_model_backward(AL, Y, caches):
    grads = {}
    L = len(caches) # the number of layers
    m = AL.shape[1]
    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    current_cache = caches[L-1]
    grads["dA" + str(L-1)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL,current_cache,"sigmoid")

    for l in reversed(range(L-1)):
        current_cache = caches[l]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(l+1)],current_cache,"relu")
        grads["dA" + str(l)] = dA_prev_temp
        grads["dW" + str(l + 1)] = dW_temp
        grads["db" + str(l + 1)] = db_temp

    return grads
</code></pre>

<pre><code class="language-python">def update_parameters(parameters, grads, learning_rate):
    L = len(parameters) // 2 # number of layers in the neural network
    for l in range(L):
        parameters["W" + str(l+1)] += - learning_rate*grads["dW" + str(l+1)]
        parameters["b" + str(l+1)] += - learning_rate*grads["db" + str(l+1)]
    return parameters
</code></pre>

<pre><code class="language-python">
</code></pre>
</p>
	</div>

</section>
</div>


<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				
				
				<li><a href="http://weibo.com/u/2377304267" class="icon alt fa-weibo" target="_blank"><span class="label">weibo</span></a></li>
				
				
				<li><a href="https://www.facebook.com/profile.php?id=100009585159226" class="icon alt fa-facebook" target="_blank"><span class="label">Facebook</span></a></li>
				
				
				<li><a href="https://www.instagram.com/harryliuxin/" class="icon alt fa-instagram" target="_blank"><span class="label">Instagram</span></a></li>
				
				
				
				
				
				<li><a href="http://www.miaopai.com/u/wxsso_p4mslofx14" class="icon alt fa-video-camera" target="_blank"><span class="label">video</span></a></li>
				
				
				<li><a href="https://www.linkedin.com/in/xin-liu-868501134" class="icon alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
				
			</ul>
			<ul class="copyright">
				<li>&copy; LiuXin </li>
				<li> stay hungry, stay foolish</li>
				<li>Designed by: <a href="https://liuxin.in/about/" target="_blank">刘鑫</a></li>

			</ul>
		</div>
	</footer>

	

</body>

</html>