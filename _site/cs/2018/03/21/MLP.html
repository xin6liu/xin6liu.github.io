<!DOCTYPE html>

<html>

<head>
	<title>LiuXin</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/assets/css/main_new.css">
	<!--[if lte IE 9]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="http://localhost:4000/assets/css/ie8.css" /><![endif]-->
	<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>


<body>

<!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header"	
					>
	<a href="http://localhost:4000/" class="logo"><strong>LiuXin</strong> <span>stay hungry, stay foolish</span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
</header>

<!-- Menu -->
<nav id="menu">
	<ul class="links">
		<!-- 
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		 -->
        <!-- 
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000/">Home</a></li>
	    	
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		
		    
		 -->
		
		
        
		<li><a href="http://localhost:4000/">网页开发 Web Development</a></li>
        
		
        
		<li><a href="http://localhost:4000/">机器学习 Machine Learning</a></li>
        
		
        
		<li><a href="http://localhost:4000/">计算流体力学 CFD</a></li>
        
		
        
		<li><a href="http://localhost:4000/">计算机科学 Computer Science</a></li>
        
		
        
		<li><a href="http://localhost:4000/">纳米科学 Nanoscience</a></li>
        
		
        
		<li><a href="http://localhost:4000/">课程 Course</a></li>
        
		
        
		<li><a href="http://localhost:4000/">生活 Diary</a></li>
        
		
        
		<li><a href="http://localhost:4000/">总目录 Categories</a></li>
        
		
		

		<!-- 
		    
		        <li><a href="http://localhost:4000/home/1python.html">网页开发 Web Development</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/2ai.html">机器学习 Machine Learning</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/3cfd.html">计算流体力学 CFD</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/4cs.html">计算机科学 Computer Science</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/5mns.html">纳米科学 Nanoscience</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/6course.html">课程 Course</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/7diary.html">生活 Diary</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/8categories.html">总目录 Categories</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/file/3cfd/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/PDE">PDE</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/PDE/PDE%E5%9F%BA%E7%A1%80">PDE基础</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/file/about.html">我的故事 My Stories</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/file/category.html">目录 Category</a></li>
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000/file/3cfd/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E5%8F%98%E5%88%86%E6%B3%95">变分法</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/file/3cfd/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E6%9C%89%E9%99%90%E4%BD%93%E7%A7%AF%E6%B3%95">有限体积法</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E6%9C%89%E9%99%90%E5%85%83%E6%B3%95">有限元法</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6">流体力学</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E6%9C%89%E9%99%90%E4%BD%93%E7%A7%AF%E6%B3%95/%E7%A6%BB%E6%95%A3%E6%96%B9%E6%B3%95">离散方法</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E8%BF%9E%E7%BB%AD%E4%BB%8B%E8%B4%A8%E5%8A%9B%E5%AD%A6">连续介质力学</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/%E7%9B%AE%E5%BD%95catalog/%E8%AE%A1%E7%AE%97%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/%E6%9C%89%E9%99%90%E4%BD%93%E7%A7%AF%E6%B3%95/%E9%80%9F%E5%BA%A6%E5%8E%8B%E5%8A%9B%E8%80%A6%E5%90%88%E6%96%B9%E6%B3%95">速度压力耦合方法</a></li>
		    
		 -->
	</ul>
	<!-- <ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul> -->
</nav> 
    
    
<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
<!-- title -->
			<h1>MLP</h1>
<!-- 日期 -->		
		
		2018-03-21
		
		</header>
<!-- description -->	
		<h5></h5>
<!-- 图片 -->		
		<!-- 
		 -->
<!-- 正文 -->
		<p><p><strong>多层感知器</strong>（Multilayer Perceptron,缩写MLP）是一种最常见的前向结构的人工神经网络(前馈神经网络)。</p>

<p><strong>反向传播算法</strong>的监督学习方法常被用来训练MLP, 下面是反向传播算法的笔记。</p>

<blockquote>
  <p>传送门：</p>

  <p><a href="http://liuxin21.com/python,%20cs/2018/03/12/sklearn.html">numpy, pandas, seaborn, sklearn 基础</a></p>

  <p><a href="http://ibillxia.github.io/blog/2013/03/24/classes-of-neural-networks/">神经网络模型分类</a></p>

  <p><a href="http://liuxin21.com/ai/2018/03/18/ai.html">神经网络基础</a></p>
</blockquote>

<p>在之前的文章中提到了神经网络<code class="highlighter-rouge">model</code>, 衡量<code class="highlighter-rouge">model</code>性能的损失函数$J(\theta)$, 使$J(\theta)$减小的学习算法<code class="highlighter-rouge">learn</code>(梯度下降算法)。</p>

<p>这一次我们来探讨<strong>梯度下降</strong>这个<code class="highlighter-rouge">learn</code>中，如何求$J(\theta)$的梯度。</p>

<p>我们用到的就是——<strong>反向传播算法（backpropogate algorithm）</strong>。</p>

<h2 id="反向传播算法-和-链式法则">反向传播算法 和 “链式法则”</h2>

<p>我们考虑只有两个网络层和一个输出层，第一层（输入层）只有两个节点，第二层只有一个节点的情况：</p>

<p><img src="https://i.imgur.com/lPgoVkz.png" alt="Screenshot 2018-03-21 16.43.56" /></p>

<p>其中，$\theta$ 为 $w_{11}, w_{12}, \mathrm{bias}_1$ 中的一个，$x$ 为
$\left[
\begin{matrix}
a_1 <br />
a_2 
\end{matrix}
\right]$
。
根据链式法则：</p>

<p><img src="https://i.imgur.com/GFY4Jy7.png" alt="Screenshot 2018-03-21 16.44.16" /></p>

<p>其中，</p>

<p><img src="https://i.imgur.com/MG2BDUZ.png" alt="Screenshot 2018-03-21 16.53.44" /></p>

<p>举个栗子：</p>

<p><img src="https://i.imgur.com/ObTOvDi.png" alt="Screenshot 2018-03-21 16.44.16 copy" /></p>

<h2 id="python-代码实现">Python 代码实现</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">C1</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lx</span><span class="p">,</span> <span class="n">ly</span><span class="p">):</span>
        <span class="s">'''
        这里的输入层(a1,a2)的长度lx=2, 输出层(b1)的长度ly=1
        weight矩阵就是1X2, bias就是1X1 
        '''</span>   
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">ly</span><span class="p">,</span><span class="n">lx</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">ly</span><span class="p">)</span>
    

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">'''
        输入：输入层x(a1,a2)
        返回：输出层y(b1)
        '''</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
        <span class="s">'''
        输入：导数值d
        返回：参数w的梯度dw
        '''</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dw</span> <span class="o">=</span> <span class="n">d</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span>
        <span class="c"># self.db = d</span>
        <span class="c"># self.dx = d * self.weights</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dw</span>
</code></pre></div></div>

<p>上面的代码的三个def完成了三个工作：</p>
<ol>
  <li>随机初始化网络参数w和bias</li>
  <li>输入x，计算出这层的y，向前传递给下一层</li>
  <li>将前面层向后传递的导数值与这一层的x想乘，得到最后一层对本层参数$\theta$的梯度</li>
</ol>

<p>然后进行sigmoid层：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">C2</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">sig</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dx</span> <span class="o">=</span> <span class="n">sig</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sig</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dx</span>
</code></pre></div></div>

<p>把上面两层拼起来，就完成了整体的网络结构：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">c1</span> <span class="o">=</span> <span class="n">C1</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">c2</span> <span class="o">=</span> <span class="n">C2</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">]])</span>
    <span class="k">print</span><span class="p">(</span><span class="n">c1</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">c1</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    
    <span class="n">y1</span> <span class="o">=</span> <span class="n">c1</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="n">c2</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span>
    
    <span class="n">dh</span> <span class="o">=</span> <span class="n">c2</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">c1</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dh</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">dh</span><span class="p">,</span> <span class="n">dw</span><span class="p">)</span>
    
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[-1.07906824 -0.42675882]] [ 0.2913231] [[1]
 [2]]
[[-1.64126277]] [[ 0.16229331]]
[[ 0.13595419]] [[ 0.13595419]
 [ 0.27190838]]
</code></pre></div></div>
</p>
	</div>

</section>
</div>


<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				
				
				<li><a href="http://weibo.com/u/2377304267" class="icon alt fa-weibo" target="_blank"><span class="label">weibo</span></a></li>
				
				
				<li><a href="https://www.facebook.com/profile.php?id=100009585159226" class="icon alt fa-facebook" target="_blank"><span class="label">Facebook</span></a></li>
				
				
				<li><a href="https://www.instagram.com/harryliuxin/" class="icon alt fa-instagram" target="_blank"><span class="label">Instagram</span></a></li>
				
				
				
				
				
				<li><a href="http://www.miaopai.com/u/wxsso_p4mslofx14" class="icon alt fa-video-camera" target="_blank"><span class="label">video</span></a></li>
				
				
				<li><a href="https://www.linkedin.com/in/xin-liu-868501134" class="icon alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
				
			</ul>
			<ul class="copyright">
				<li>&copy; LiuXin </li>
				<li> stay hungry, stay foolish</li>
				<li>Designed by: <a href="https://liuxin.in/about/" target="_blank">刘鑫</a></li>

			</ul>
		</div>
	</footer>

	

</body>

</html>